{
  "filename": "Generative AI Concepts (1).docx",
  "fileType": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
  "extension": "docx",
  "size": 25097,
  "lastModified": "2025-11-27T05:14:27.173Z",
  "extractedAt": "2025-11-27T10:07:23.987Z",
  "data": {
    "type": "unstructured_text",
    "lineCount": 237,
    "content": "Generative AI Concepts\n\n\n\n1. Natural Language Processing (NLP)\n\nOverview\n\nNatural Language Processing (NLP) is a field of AI that enables machines to understand, interpret, and generate human language.It‚Äôs the foundation for most Generative AI applications ‚Äî from chatbots to document summarization.\n\n\n\nA. Text Processing\n\nBefore any model can process text, raw data must be cleaned and standardized.\n\nSteps in Text Preprocessing\n\nLowercasingConverts all characters to lowercase to avoid treating words like ‚ÄúChatGPT‚Äù and ‚Äúchatgpt‚Äù differently.\n\ntext = text.lower()\n\nRemoving PunctuationRemoves characters like !, ,, . which carry less meaning for models.\n\nimport string\n\ntext = text.translate(str.maketrans('', '', string.punctuation))\n\nRemoving StopwordsEliminates common words such as the, is, and, a that don‚Äôt add semantic value.\n\nfrom nltk.corpus import stopwords\n\nwords = [w for w in text.split() if w not in stopwords.words('english')]\n\nStemmingReduces words to their root form ‚Äî e.g., running ‚Üí run (uses heuristics).\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n\nstemmed = [stemmer.stem(word) for word in words]\n\nLemmatizationConverts words to their dictionary form ‚Äî e.g., better ‚Üí good (uses vocabulary + context).\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nlemmatized = [lemmatizer.lemmatize(word) for word in words]\n\n\n\nB. Tokenization\n\nTokenization breaks text into smaller units (tokens) such as words, subwords, or sentences.\n\nTypes of Tokenizers\n\nType\n\nExample\n\nUsed In\n\nWord Tokenizer\n\n[\"I\", \"love\", \"AI\"]\n\nSimple models\n\nSubword Tokenizer\n\n[\"play\", \"##ing\"]\n\nTransformers (BERT, GPT)\n\nSentence Tokenizer\n\n[\"I love AI.\", \"It‚Äôs powerful.\"]\n\nSummarization, translation\n\nTokenization prepares text for embedding generation and model input.\n\n\n\nC. Embeddings\n\nEmbeddings convert words or sentences into numerical vectors that capture meaning.Words with similar meanings will have similar vector representations.\n\n Workflow of Embedding Generation\n\nInput text ‚Üí Tokenization\n\nTokens ‚Üí Converted to numeric IDs\n\nEach ID ‚Üí Mapped to an embedding vector (dense representation)\n\nThese vectors represent semantic meaning in continuous space.\n\nExample:\n\nking - man + woman ‚âà queen\n\n Common Embedding Models\n\nWord2Vec\n\nGloVe\n\nFastText\n\nBERT embeddings\n\nSentence Transformers\n\n\n\n2. Attention Mechanism\n\nDefinition\n\nThe Attention Mechanism helps models ‚Äúfocus‚Äù on the most relevant parts of input data when generating or understanding language.\n\nInstead of treating all words equally, attention assigns scores (weights) to highlight important words.\n\n\n\n How It Works\n\nStep 1Ô∏è: Calculate Attention Scores\n\nFor each word in a sentence:\n\nCompute how much attention it should give to other words.\n\nUse dot-product similarity between query and key vectors.\n\nFormula:[\\text{score}(Q, K) = Q \\cdot K^T]\n\nStep 2Ô∏è: Normalize Scores\n\nApply Softmax to get probabilities:[\\alpha_i = \\frac{e^{score_i}}{\\sum_j e^{score_j}}]\n\nStep 3Ô∏è  :  Adjust Focus Based on Scores\n\nMultiply the scores with value vectors to generate the context vector:[\\text{Attention}(Q,K,V) = \\text{Softmax}(QK^T)V]\n\nThis gives a weighted representation of information most relevant to the current word.\n\n\n\n3. Transformers\n\nDefinition\n\nTransformers are deep learning architectures that rely entirely on attention mechanisms (no recurrence or convolution).They are the backbone of models like BERT, GPT, T5, etc.\n\n\n\n\n\n\n\n\n\n\n\nComparison: Older vs Newer NLP Models\n\nModel Type\n\nExamples\n\nLimitations\n\nTransformers Fix\n\nTraditional (RNN, LSTM, GRU)\n\nSeq2Seq, LSTM\n\nSequential ‚Üí slow, hard to parallelize\n\nParallelized processing\n\nTransformers\n\nBERT, GPT\n\nUse self-attention ‚Üí process entire context simultaneously\n\nBetter context understanding, faster training\n\n\n\nHow Transformers Work (Step-by-Step)\n\nInput Tokenization ‚Üí Convert text to tokens\n\nEmbedding Layer ‚Üí Each token ‚Üí embedding vector\n\nPositional Encoding ‚Üí Adds order information (since transformer has no sequence memory)\n\nSelf-Attention Layer ‚Üí Calculates relationships between all tokens\n\nFeed Forward Network ‚Üí Processes each token independently\n\nNormalization + Residual Connections ‚Üí Stability and gradient flow\n\nStack Multiple Layers ‚Üí Deep understanding of context\n\nOutput Layer ‚Üí Generates or classifies text\n\n\n\n4. Text Similarity\n\nText similarity measures how close two pieces of text are in meaning.\n\nMethod\n\nDescription\n\nFormula / Idea\n\nCosine Similarity\n\nMeasures angle between two vectors\n\n(\\cos(\\theta) = \\frac{A \\cdot B}{\n\nJaccard Similarity\n\nMeasures overlap between sets\n\n(J(A,B) = \\frac{\n\nEuclidean Distance\n\nMeasures straight-line distance between vectors\n\n(d(A,B) = \\sqrt{\\sum (A_i - B_i)^2})\n\n\n\n5. Information Retrieval (IR)\n\nIR focuses on finding relevant documents from a large collection.\n\nüîπ Components\n\nDocument Representation\n\nConvert text into vectors using TF-IDF, BM25, or embeddings.\n\nIndexing\n\nStore document vectors in searchable structure.\n\nScoring & Ranking\n\nCalculate similarity between query and document.\n\nRank documents based on relevance score.\n\n\n\n6. Introduction to RAG (Retrieval-Augmented Generation)\n\nRAG combines retrieval (finding facts) and generation (creating natural language output).\n\nWorkflow:\n\nRetrieve relevant information from external sources.\n\nAugment the prompt with retrieved data.\n\nGenerate coherent responses using a generative model (like GPT).\n\n‚úÖ This improves factual accuracy and reduces hallucination.\n\n\n\n7. Retrieval Models\n\n Definition\n\nRetrieval models find the most relevant documents for a query.\n\nTypes\n\nTraditional (Sparse)\n\nDense (Neural)\n\n\n\n Traditional Models\n\n(a) TF-IDF\n\nRepresents text as word frequencies.\n\nAssigns higher weight to words that appear frequently in a document but rarely in others.\n\nLimitations:\n\nIgnores meaning/synonyms.\n\nSparse, large vectors.\n\n\n\n(b) BM25\n\nAn improved TF-IDF variant.\n\nConsiders term frequency saturation and document length normalization.\n\nBenefits:\n\nMore accurate for keyword-based search.\n\nStrong baseline in IR systems.\n\nLimitations:\n\nStill purely lexical ‚Äî can‚Äôt understand semantics.\n\n\n\nDense Retrieval Models\n\nUse neural networks to represent documents and queries in a shared embedding space.\n\nWorkflow:\n\nEncode query ‚Üí embedding\n\nEncode documents ‚Üí embeddings\n\nCompute cosine similarity\n\nRetrieve top-k most similar documents\n\n‚úÖ Benefits:\n\nUnderstand semantic meaning\n\nSupports contextual queries\n\n‚ö†Ô∏è Limitations:\n\nComputationally heavy\n\nRequires large training data\n\n\n\nAbsolutely üëç ‚Äî here‚Äôs a concise, clear version of the DPR (Dense Passage Retrieval) workflow, perfect for your documentation:\n\n\n\nDPR (Dense Passage Retrieval)\n\nDefinition:DPR is a neural retrieval model that finds semantically relevant passages for a given query using dense embeddings instead of keyword matching.\n\n\n\nWorkflow Steps\n\nInput:\n\nTakes a query (e.g., ‚ÄúWho founded PostgreSQL?‚Äù)\n\nAnd a collection of passages/documents\n\nEncoders:\n\nQuery Encoder ‚Üí converts query into a dense vector\n\nPassage Encoder ‚Üí converts each passage into a dense vector\n\nBoth are typically BERT-based and map text into the same vector space.\n\nCompute Similarity:\n\nCalculate cosine similarity between query and passage vectors\n\nHigher score = higher semantic relevance\n\nRank Documents:\n\nRank all passages based on similarity scores\n\nSelect the Top-K most relevant ones\n\nOutput:\n\nReturns top passages as retrieved context for RAG or QA systems\n\nThese passages are used by the generator (like GPT) to produce final, factual responses.\n\n\n\n8. Generative Models\n\nDefinition\n\nGenerative models produce new text by predicting the next token based on previous context (e.g., GPT, T5, LLaMA).\n\nAdvantages over Retrieval Models\n\nRetrieval Models\n\nGenerative Models\n\nFetch existing data\n\nGenerate new data\n\nKeyword or embedding-based\n\nContext and pattern-based\n\nStatic knowledge\n\nCan generalize & reason\n\nLimited creativity\n\nHighly flexible & adaptive\n\n\n\n Workflow of Generative Models\n\nTokenize input text\n\nPass through transformer layers\n\nCompute attention & hidden states\n\nPredict next token (autoregressive generation)\n\nRepeat until complete output\n\n\n\nComparison: Retrieval vs Generative\n\nFeature\n\nRetrieval\n\nGenerative\n\nOutput\n\nFactual text from memory\n\nNew content\n\nContext\n\nNarrow\n\nDeep contextual\n\nExample\n\nSearch engine\n\nChatGPT\n\nLimitation\n\nCan‚Äôt create new info\n\nMay hallucinate\n\nBest Use\n\nInformation lookup\n\nConversation, summarization\n\n\n\n\n\n\n\n9. Combining Retrieval + Generation (RAG Systems)\n\n Why Combine?\n\nBecause:\n\nRetrieval ensures accuracy & factual grounding\n\nGeneration ensures natural, coherent language\n\n\n\nSteps in Retrieval\n\nRepresent query\n\nSearch index\n\nScore and rank results\n\nReturn top relevant documents\n\nSteps in Generation\n\nTake retrieved documents\n\nAppend to user query (context window)\n\nGenerate text response using LLM\n\nRAG = Retrieval + Generation\n\nQuery ‚Üí Retrieve supporting facts\n\nFused prompt ‚Üí Pass to LLM\n\nLLM ‚Üí Generate answer with evidence\n\n‚úÖ Result: Factual, context-aware, and fluent outputs.\n\n",
    "detectedPatterns": [
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value"
    ],
    "keyValuePairs": {
      "Step 1Ô∏è": "Calculate Attention Scores",
      "Formula": "[\\text{score}(Q, K) = Q \\cdot K^T]",
      "Step 2Ô∏è": "Normalize Scores",
      "Apply Softmax to get probabilities": "[\\alpha_i = \\frac{e^{score_i}}{\\sum_j e^{score_j}}]",
      "Step 3Ô∏è": "Adjust Focus Based on Scores",
      "Multiply the scores with value vectors to generate the context vector": "[\\text{Attention}(Q,K,V) = \\text{Softmax}(QK^T)V]",
      "Comparison": "Retrieval vs Generative",
      "Definition": "DPR is a neural retrieval model that finds semantically relevant passages for a given query using dense embeddings instead of keyword matching.",
      "‚úÖ Result": "Factual, context-aware, and fluent outputs."
    }
  }
}