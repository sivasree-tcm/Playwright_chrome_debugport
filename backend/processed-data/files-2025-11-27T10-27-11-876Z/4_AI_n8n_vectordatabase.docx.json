{
  "filename": "AI_n8n_vectordatabase.docx",
  "fileType": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
  "extension": "docx",
  "size": 2381769,
  "lastModified": "2025-10-15T12:35:06.390Z",
  "extractedAt": "2025-11-27T10:27:11.144Z",
  "data": {
    "type": "unstructured_text",
    "lineCount": 364,
    "content": "Generative AI Concepts\n\n1. Natural Language Processing (NLP)\n\nOverview\n\nNatural Language Processing (NLP) is a field of AI that enables machines to understand, interpret, and generate human language.It‚Äôs the foundation for most Generative AI applications ‚Äî from chatbots to document summarization.\n\n\n\nA. Text Processing\n\nBefore any model can process text, raw data must be cleaned and standardized.\n\nSteps in Text Preprocessing\n\nLowercasingConverts all characters to lowercase to avoid treating words like ‚ÄúChatGPT‚Äù and ‚Äúchatgpt‚Äù differently.\n\ntext = text.lower()\n\nRemoving PunctuationRemoves characters like !, ,, . which carry less meaning for models.\n\nimport string\n\ntext = text.translate(str.maketrans('', '', string.punctuation))\n\nRemoving StopwordsEliminates common words such as the, is, and, a that don‚Äôt add semantic value.\n\nfrom nltk.corpus import stopwords\n\nwords = [w for w in text.split() if w not in stopwords.words('english')]\n\nStemmingReduces words to their root form ‚Äî e.g., running ‚Üí run (uses heuristics).\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n\nstemmed = [stemmer.stem(word) for word in words]\n\nLemmatizationConverts words to their dictionary form ‚Äî e.g., better ‚Üí good (uses vocabulary + context).\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nlemmatized = [lemmatizer.lemmatize(word) for word in words]\n\n\n\nB. Tokenization\n\nTokenization breaks text into smaller units (tokens) such as words, subwords, or sentences.\n\nTypes of Tokenizers\n\nType\n\nExample\n\nUsed In\n\nWord Tokenizer\n\n[\"I\", \"love\", \"AI\"]\n\nSimple models\n\nSubword Tokenizer\n\n[\"play\", \"##ing\"]\n\nTransformers (BERT, GPT)\n\nSentence Tokenizer\n\n[\"I love AI.\", \"It‚Äôs powerful.\"]\n\nSummarization, translation\n\nTokenization prepares text for embedding generation and model input.\n\n\n\nC. Embeddings\n\nEmbeddings convert words or sentences into numerical vectors that capture meaning.Words with similar meanings will have similar vector representations.\n\n Workflow of Embedding Generation\n\nInput text ‚Üí Tokenization\n\nTokens ‚Üí Converted to numeric IDs\n\nEach ID ‚Üí Mapped to an embedding vector (dense representation)\n\nThese vectors represent semantic meaning in continuous space.\n\nExample:\n\nking - man + woman ‚âà queen\n\n Common Embedding Models\n\nWord2Vec\n\nGloVe\n\nFastText\n\nBERT embeddings\n\nSentence Transformers\n\n\n\n2. Attention Mechanism\n\nDefinition\n\nThe Attention Mechanism helps models ‚Äúfocus‚Äù on the most relevant parts of input data when generating or understanding language.\n\nInstead of treating all words equally, attention assigns scores (weights) to highlight important words.\n\n\n\n How It Works\n\nStep 1Ô∏è: Calculate Attention Scores\n\nFor each word in a sentence:\n\nCompute how much attention it should give to other words.\n\nUse dot-product similarity between query and key vectors.\n\nFormula:[\\text{score}(Q, K) = Q \\cdot K^T]\n\nStep 2Ô∏è: Normalize Scores\n\nApply Softmax to get probabilities:[\\alpha_i = \\frac{e^{score_i}}{\\sum_j e^{score_j}}]\n\nStep 3Ô∏è  :  Adjust Focus Based on Scores\n\nMultiply the scores with value vectors to generate the context vector:[\\text{Attention}(Q,K,V) = \\text{Softmax}(QK^T)V]\n\nThis gives a weighted representation of information most relevant to the current word.\n\n\n\n3. Transformers\n\nDefinition\n\nTransformers are deep learning architectures that rely entirely on attention mechanisms (no recurrence or convolution).They are the backbone of models like BERT, GPT, T5, etc.\n\n\n\n\n\n\n\n\n\n\n\nComparison: Older vs Newer NLP Models\n\nModel Type\n\nExamples\n\nLimitations\n\nTransformers Fix\n\nTraditional (RNN, LSTM, GRU)\n\nSeq2Seq, LSTM\n\nSequential ‚Üí slow, hard to parallelize\n\nParallelized processing\n\nTransformers\n\nBERT, GPT\n\nUse self-attention ‚Üí process entire context simultaneously\n\nBetter context understanding, faster training\n\n\n\nHow Transformers Work (Step-by-Step)\n\nInput Tokenization ‚Üí Convert text to tokens\n\nEmbedding Layer ‚Üí Each token ‚Üí embedding vector\n\nPositional Encoding ‚Üí Adds order information (since transformer has no sequence memory)\n\nSelf-Attention Layer ‚Üí Calculates relationships between all tokens\n\nFeed Forward Network ‚Üí Processes each token independently\n\nNormalization + Residual Connections ‚Üí Stability and gradient flow\n\nStack Multiple Layers ‚Üí Deep understanding of context\n\nOutput Layer ‚Üí Generates or classifies text\n\n\n\n4. Text Similarity\n\nText similarity measures how close two pieces of text are in meaning.\n\nMethod\n\nDescription\n\nFormula / Idea\n\nCosine Similarity\n\nMeasures angle between two vectors\n\n(\\cos(\\theta) = \\frac{A \\cdot B}{\n\nJaccard Similarity\n\nMeasures overlap between sets\n\n(J(A,B) = \\frac{\n\nEuclidean Distance\n\nMeasures straight-line distance between vectors\n\n(d(A,B) = \\sqrt{\\sum (A_i - B_i)^2})\n\n\n\n5. Information Retrieval (IR)\n\nIR focuses on finding relevant documents from a large collection.\n\nüîπ Components\n\nDocument Representation\n\nConvert text into vectors using TF-IDF, BM25, or embeddings.\n\nIndexing\n\nStore document vectors in searchable structure.\n\nScoring & Ranking\n\nCalculate similarity between query and document.\n\nRank documents based on relevance score.\n\n\n\n6. Introduction to RAG (Retrieval-Augmented Generation)\n\nRAG combines retrieval (finding facts) and generation (creating natural language output).\n\nWorkflow:\n\nRetrieve relevant information from external sources.\n\nAugment the prompt with retrieved data.\n\nGenerate coherent responses using a generative model (like GPT).\n\n‚úÖ This improves factual accuracy and reduces hallucination.\n\n\n\n7. Retrieval Models\n\n Definition\n\nRetrieval models find the most relevant documents for a query.\n\nTypes\n\nTraditional (Sparse)\n\nDense (Neural)\n\n\n\n Traditional Models\n\n(a) TF-IDF\n\nRepresents text as word frequencies.\n\nAssigns higher weight to words that appear frequently in a document but rarely in others.\n\nLimitations:\n\nIgnores meaning/synonyms.\n\nSparse, large vectors.\n\n\n\n(b) BM25\n\nAn improved TF-IDF variant.\n\nConsiders term frequency saturation and document length normalization.\n\nBenefits:\n\nMore accurate for keyword-based search.\n\nStrong baseline in IR systems.\n\nLimitations:\n\nStill purely lexical ‚Äî can‚Äôt understand semantics.\n\n\n\nDense Retrieval Models\n\nUse neural networks to represent documents and queries in a shared embedding space.\n\nWorkflow:\n\nEncode query ‚Üí embedding\n\nEncode documents ‚Üí embeddings\n\nCompute cosine similarity\n\nRetrieve top-k most similar documents\n\n‚úÖ Benefits:\n\nUnderstand semantic meaning\n\nSupports contextual queries\n\n‚ö†Ô∏è Limitations:\n\nComputationally heavy\n\nRequires large training data\n\n\n\nAbsolutely üëç ‚Äî here‚Äôs a concise, clear version of the DPR (Dense Passage Retrieval) workflow, perfect for your documentation:\n\n\n\nDPR (Dense Passage Retrieval)\n\nDefinition:DPR is a neural retrieval model that finds semantically relevant passages for a given query using dense embeddings instead of keyword matching.\n\n\n\nWorkflow Steps\n\nInput:\n\nTakes a query (e.g., ‚ÄúWho founded PostgreSQL?‚Äù)\n\nAnd a collection of passages/documents\n\nEncoders:\n\nQuery Encoder ‚Üí converts query into a dense vector\n\nPassage Encoder ‚Üí converts each passage into a dense vector\n\nBoth are typically BERT-based and map text into the same vector space.\n\nCompute Similarity:\n\nCalculate cosine similarity between query and passage vectors\n\nHigher score = higher semantic relevance\n\nRank Documents:\n\nRank all passages based on similarity scores\n\nSelect the Top-K most relevant ones\n\nOutput:\n\nReturns top passages as retrieved context for RAG or QA systems\n\nThese passages are used by the generator (like GPT) to produce final, factual responses.\n\n\n\n8. Generative Models\n\nDefinition\n\nGenerative models produce new text by predicting the next token based on previous context (e.g., GPT, T5, LLaMA).\n\nAdvantages over Retrieval Models\n\nRetrieval Models\n\nGenerative Models\n\nFetch existing data\n\nGenerate new data\n\nKeyword or embedding-based\n\nContext and pattern-based\n\nStatic knowledge\n\nCan generalize & reason\n\nLimited creativity\n\nHighly flexible & adaptive\n\n\n\n Workflow of Generative Models\n\nTokenize input text\n\nPass through transformer layers\n\nCompute attention & hidden states\n\nPredict next token (autoregressive generation)\n\nRepeat until complete output\n\n\n\nComparison: Retrieval vs Generative\n\nFeature\n\nRetrieval\n\nGenerative\n\nOutput\n\nFactual text from memory\n\nNew content\n\nContext\n\nNarrow\n\nDeep contextual\n\nExample\n\nSearch engine\n\nChatGPT\n\nLimitation\n\nCan‚Äôt create new info\n\nMay hallucinate\n\nBest Use\n\nInformation lookup\n\nConversation, summarization\n\n\n\n\n\n\n\n9. Combining Retrieval + Generation (RAG Systems)\n\n Why Combine?\n\nBecause:\n\nRetrieval ensures accuracy & factual grounding\n\nGeneration ensures natural, coherent language\n\n\n\nSteps in Retrieval\n\nRepresent query\n\nSearch index\n\nScore and rank results\n\nReturn top relevant documents\n\nSteps in Generation\n\nTake retrieved documents\n\nAppend to user query (context window)\n\nGenerate text response using LLM\n\nRAG = Retrieval + Generation\n\nQuery ‚Üí Retrieve supporting facts\n\nFused prompt ‚Üí Pass to LLM\n\nLLM ‚Üí Generate answer with evidence\n\n‚úÖ Result: Factual, context-aware, and fluent outputs.\n\n\n\nN8N:\n\nn8n is a workflow automation tool that lets  connect apps and services without writing complex code. It‚Äôs like a visual builder for automating tasks ‚Äî perfect for NGOs, SHGs, and small teams.\n\nUse Case\n\nWhat n8n Can Automate\n\nGrant Alerts\n\nFetch new grants from websites or emails and send updates to WhatsApp, Telegram, or email\n\nEmail Parsing\n\nExtract keywords, deadlines, and links from incoming emails\n\nTranslation\n\nAutomatically translate alerts into Tamil using AI or APIs\n\nVoice Playback\n\nConvert grant summaries into audio using text-to-speech tools\n\nDashboard Sync\n\nPush data into Google Sheets, Airtable, or Streamlit dashboards\n\nReminders\n\nSchedule follow-ups or deadline alerts for field teams\n\nForm Collection\n\nCollect responses from Typeform, Google Forms, or WhatsApp and store them in a database\n\nÔÇ∑  Nodes: Each step in your workflow (e.g., \"Get email\", \"Translate text\", \"Send WhatsApp\")\n\nÔÇ∑  Triggers: Start the workflow (e.g., \"New email received\", \"Every morning at 9 AM\")\n\nÔÇ∑  Credentials: Securely connect to services like Gmail, Telegram, Airtable, etc.\n\nÔÇ∑  Variables: Pass data between steps (e.g., grant title, deadline, link)\n\nProject 1:\n\nThis project automatically handles form submissions and organizes responses. When someone fills out a form‚Äîsuch as a grant interest or contact form‚Äîthe workflow starts immediately. It collects the submitted details like name, organization, and message. Then, it sends a personalized email reply to confirm receipt and thank the person. At the same time, the form data is added to a Google Sheet, so your team can track and follow up easily. This saves time, avoids manual work, and keeps everything organized.\n\nWorkflow Overview\n\nTrigger ‚Üí Form Submission ‚Üí Email Response ‚Üí Google Sheets Entry\n\n\n\n\n\n\n\n\n\n\n\nProject 2: \n\nAutomated Workflow in n8n for Scheduled Data Retrieval, Filtering, Alerts, and Google Sheet Logging\n\nSchedule Trigger The workflow starts automatically at a set time ‚Äî for example, every morning or once a week.\n\nHTTP Request It sends a GET request to an external API (https://appapi.com/space) to fetch data ‚Äî maybe space news, updates, or satellite info.\n\nPython Code (Beta) Custom Python code is used to process or clean the data ‚Äî like extracting key fields, formatting text, or filtering results.\n\nEdit Fields This step lets you manually adjust or enrich the data ‚Äî for example, adding a label, correcting a value, or tagging it for a specific use.\n\nSend a Message (Gmail) The processed data is sent as an email ‚Äî maybe to notify a team, share a summary, or alert someone about new info.\n\nAppend Row in Google Sheets Finally, the data is saved in a Google Sheet ‚Äî creating a permanent record that can be tracked, filtered, or visualized later.\n\n\n\n\n\n\n\n\n\nProject 3\n\nThis workflow is designed to automatically process files uploaded to Google Drive and make them searchable through an AI-powered chat system. It begins when a new file is created in a specific Google Drive folder. The system immediately downloads the file and stores it in a local folder. Then, it reads the content of the file and converts it into embeddings using Ollama ‚Äî a model that transforms text into a format that can be searched semantically. These embeddings are saved in a vector store, which acts like a smart database that understands meaning, not just keywords. When a user sends a message or asks a question through the chat interface, the AI agent is triggered. It uses the stored embeddings to find the most relevant information from the uploaded files. The agent then replies with a clear, context-aware answer based on the content. This setup allows users to ask questions about any document in the folder and get instant, intelligent responses ‚Äî perfect for grant tracking, report analysis, or multilingual document support.\n\n\n\nVector Database:\n\nA vector database is specifically designed to store and search high-dimensional vectors ‚Äî like word embeddings, image embeddings, or audio embeddings.\n\n                         \n\nTraditional Database:\n\nMost real-world data is unstructured, and storing it in traditional databases like MySQL, PostgreSQL, or SQLite comes with limitations.\n\nUnstructured data includes:\n\n Images\n\nAudio files\n\nPDFs and scanned documents\n\nFree-form text (emails, notes, social media posts)\n\nIt doesn‚Äôt fit neatly into rows and columns like structured data (e.g., names, dates, numbers).\n\nVector Embeddings:\n\nA vector embedding is:A numerical fingerprint of something unstructured.Instead of storing raw text like \"school grant for rural girls\", we convert it into a vector like:\n\n[0.12, -0.45, 0.88, ..., 0.03]  ‚Üê (e.g., 384 or 768 numbers)\n\n\n\n\n\nVector embeddings are a way to turn complex data like text, images, or audio into a list of numbers that capture their meaning. Instead of just storing raw words or files, we use AI models to convert them into these numerical representations, which are called embeddings. These embeddings help us compare and search data based on similarity ‚Äî for example, finding documents that talk about the same topic even if they use different words. Once we have these embeddings, we store them in a special type of database called a vector database, which is designed to handle high-dimensional data and perform fast, meaning-based searches. This is especially useful for building smart applications like grant search tools, chatbots, or recommendation systems that need to understand and retrieve information based on context, not just exact matches.\n\nHow Embedding Model Work:\n\n\n\n\n\n\n\nHow Word Embeddings Are Created and Used\n\nLearning from Language AI models (like BERT or OpenAI‚Äôs embedding models) read millions of sentences and learn how words relate to each other ‚Äî like how ‚Äúking‚Äù often appears near ‚Äúroyal‚Äù or ‚Äúpower.‚Äù\n\nTurning Words into Numbers Each word is converted into a vector ‚Äî a long list of numbers (like [0.12, -0.45, 0.88, ...]) that captures its meaning, context, and relationships.\n\nPlacing Words in Space These vectors are plotted in a multi-dimensional space. Words with similar meanings (like ‚Äúking‚Äù and ‚Äúqueen‚Äù) end up close together. Words with different meanings (like ‚Äúmonkey‚Äù and ‚Äúqueen‚Äù) are far apart.\n\nUsing Embeddings for Smart Search When someone types a query like ‚Äúfunds for rural schools,‚Äù the system converts it into a vector and finds documents or grants with similar vectors ‚Äî even if they don‚Äôt use the exact same words.\n\nSome Embedding Models:\n\nOpen Ai embedding\n\nGPT\n\nPAID\n\nHugging Face Embedding\n\nOpen LLm\n\nFREE\n\nLlama2 Embedding\n\nFacebook\n\nFREE\n\nGoogle Palm Embedding\n\nGoogle\n\nFREE\n\n\n\nExample:\n\n\n\n\n\n\n\nWhy Searching with Euclidean or Cosine Distance Is Slow\n\nWhen we want to find similar vectors (e.g., similar grant descriptions),  usually:\n\nUse Euclidean distance (how far apart two vectors are)\n\nOr Cosine similarity (how aligned two vectors are)\n\nBut if we have 10,000 or 1 million vectors, the system must:\n\nCompare query vector to every single stored vector\n\nCalculate the distance for each one\n\nSort and return the closest matches\n\nThis is called brute-force search ‚Äî and it‚Äôs slow for large datasets.\n\nVector Index:\n\n\n\nA vector index is a smart structure that organizes vectors so you don‚Äôt have to compare all of them.\n\nIt uses techniques like:\n\nClustering (grouping similar vectors together)\n\nGraphs (like HNSW: Hierarchical Navigable Small World)\n\nHashing (like LSH: Locality Sensitive Hashing)\n\n\n\n\n\nUseCases of Vector Database:\n\nLong-Term Memory for LLMs\n\nLarge Language Models (LLMs) like PaLM or GPT can‚Äôt remember past conversations or documents unless we give them that context again. Vector DBs act like memory ‚Äî storing embeddings of previous chats, documents, or user actions. When the model needs context, it retrieves relevant info using semantic similarity.\n\nExample: Your grant assistant remembers past queries like ‚Äúeducation funds in Tamil Nadu‚Äù and retrieves similar grants automatically.\n\nSemantic Search\n\nInstead of matching exact keywords, semantic search finds results based on meaning. Vector DBs store text as embeddings, so you can search for ‚Äúfunds for rural girls‚Äù and still find documents that say ‚Äúsupport for female students in villages.‚Äù\n\nExample: A user types in Tamil or English, and your dashboard shows relevant grants even if the words don‚Äôt match exactly.\n\nSimilarity Search (Text, Images, Videos, Audios)\n\nVector DBs can store embeddings for any media type ‚Äî not just text. You can search for similar images, voice notes, or videos by comparing their vector representations.\n\nExample: Upload a flyer or voice note, and the system finds similar grants or alerts.\n\nRecommendation Engine\n\nBy comparing user queries or behavior to stored embeddings, vector DBs can suggest related content. This powers smart recommendations ‚Äî like ‚Äúwe might also be interested in‚Ä¶‚Äù\n\nExample: After viewing a health grant, the system recommends nearby sanitation or nutrition programs.\n\nSome Vector Database:\n\nChroma\n\nFAISS\n\nWeaviate\n\nPinecone\n\nRedis\n\nMilvus\n\nQdrant\n\nVespa\n\nVald\n\nZilliz\n\nChroma\n\nChroma is the open-source AI application database. Chroma makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs\n\nInstall chroma:\n\npip install chromadb\n\nDownload articles:\n\nInvoke-WebRequest -Uri \"https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\" -OutFile \"new_articles.zip\"\n\nUnzip file:\n\nExpand-Archive -Path \"new_articles.zip\" -DestinationPath \"new_articles\"\n\nWhat is Chunking?\n\nChunking means splitting a large document into smaller pieces called chunks. These chunks are easier for AI models to understand, search, and answer questions from.\n\nWhat is Chunk Size?\n\nChunk size is the maximum number of characters allowed in each chunk.\n\nFor example:\n\nIf chunk size is set to 1000, each chunk will contain up to 1000 characters from the original text.\n\nThink of it like cutting a long rope into smaller sections‚Äîeach section is a chunk.\n\nWhat is Chunk Overlap?\n\nChunk overlap means that the end of one chunk is repeated at the beginning of the next chunk.\n\nThis helps the AI maintain context between chunks. Without overlap, important connections between sentences might be lost.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "detectedPatterns": [
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value",
      "key-value"
    ],
    "keyValuePairs": {
      "Step 1Ô∏è": "Calculate Attention Scores",
      "Formula": "[\\text{score}(Q, K) = Q \\cdot K^T]",
      "Step 2Ô∏è": "Normalize Scores",
      "Apply Softmax to get probabilities": "[\\alpha_i = \\frac{e^{score_i}}{\\sum_j e^{score_j}}]",
      "Step 3Ô∏è": "Adjust Focus Based on Scores",
      "Multiply the scores with value vectors to generate the context vector": "[\\text{Attention}(Q,K,V) = \\text{Softmax}(QK^T)V]",
      "Comparison": "Retrieval vs Generative",
      "Definition": "DPR is a neural retrieval model that finds semantically relevant passages for a given query using dense embeddings instead of keyword matching.",
      "‚úÖ Result": "Factual, context-aware, and fluent outputs.",
      "ÔÇ∑  Nodes": "Each step in your workflow (e.g., \"Get email\", \"Translate text\", \"Send WhatsApp\")",
      "ÔÇ∑  Triggers": "Start the workflow (e.g., \"New email received\", \"Every morning at 9 AM\")",
      "ÔÇ∑  Credentials": "Securely connect to services like Gmail, Telegram, Airtable, etc.",
      "ÔÇ∑  Variables": "Pass data between steps (e.g., grant title, deadline, link)",
      "Project 2": "",
      "HTTP Request It sends a GET request to an external API (https": "//appapi.com/space) to fetch data ‚Äî maybe space news, updates, or satellite info.",
      "A vector embedding is": "A numerical fingerprint of something unstructured.Instead of storing raw text like \"school grant for rural girls\", we convert it into a vector like:",
      "Graphs (like HNSW": "Hierarchical Navigable Small World)",
      "Hashing (like LSH": "Locality Sensitive Hashing)",
      "Example": "After viewing a health grant, the system recommends nearby sanitation or nutrition programs.",
      "Invoke-WebRequest -Uri \"https": "//www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\" -OutFile \"new_articles.zip\""
    }
  }
}