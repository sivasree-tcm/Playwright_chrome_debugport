{
  "totalFiles": 6,
  "successCount": 6,
  "errorCount": 0,
  "processedAt": "2025-11-27T10:45:08.245Z",
  "files": [
    {
      "filename": "1.docx",
      "fileType": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
      "extension": "docx",
      "size": 11828,
      "lastModified": "2025-10-23T12:37:14.938Z",
      "extractedAt": "2025-11-27T10:45:06.563Z",
      "data": {
        "type": "unstructured_text",
        "lineCount": 7,
        "content": "1.search for csr\n\n2.filter grants\n\n3.based on give small details\n\n4.ngo typeðŸ¡ªsummary give\n\n5.automaticatlly file form but login faced issues\n\n6.pdf alos available \n\n7.\n\n\n\n",
        "detectedPatterns": []
      }
    },
    {
      "filename": "11.docx",
      "fileType": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
      "extension": "docx",
      "size": 15776,
      "lastModified": "2025-11-19T16:08:16.284Z",
      "extractedAt": "2025-11-27T10:45:06.822Z",
      "data": {
        "type": "unstructured_text",
        "lineCount": 54,
        "content": "âŒ 1. chrome.storage has EXTREMELY small storage\n\nLimits:\n\nchrome.storage.sync â†’ 100KB total\n\nchrome.storage.local â†’ 5MB max\n\nYour NGO documents will be:\n\nPDF (2â€“20MB each)\n\nWord files\n\nImages\n\nScanned certificates\n\nâž¡ They will NOT fit in chrome.storage.Even one PDF breaks the limit.\n\n\n\nâŒ 2. chrome.storage cannot store files\n\nIt can store ONLY:\n\nstrings\n\nJSON\n\nsmall base64 data\n\nIt cannot store large PDFs or DOCX files properly.\n\nTo store 5MB PDF, you must convert to base64 â†’ becomes 7MB â†’ FAIL.\n\n\n\nâŒ 3. Data is lost if user clears browser\n\nIf user:\n\nuninstalls the extension\n\nclears cookies\n\nclears browser storage\n\nâž¡ All NGO documents are permanently lostYou have no backup.\n\nThis is VERY risky for NGO official documents.\n\n\n\nâŒ 4. chrome.storage works only on one device\n\nIf user uploads documents on:\n\nLaptop â†’ âœ“Phone â†’ âŒ (extension not synced)Another laptop â†’ âŒ\n\nThere is no multi-device syncing.\n\nNGO team members cannot share documents.\n\n\n\nâŒ 5. No security\n\nchrome.storage is stored inside the browser.\n\nAnyone can:\n\nOpen DevTools â†’ Application â†’ chrome.storage\n\nView all data\n\nEdit all data\n\nDelete all data\n\nNo encryption.No access control.Not safe for personal or NGO data.\n\n\n\nâŒ 6. No user accounts / No login\n\nYour project needs:\n\nmultiple NGOs\n\neach NGO with separate files\n\nsecure authentication\n\nrole-based access\n\nchrome.storage does not support login.Everything is stored together.\n\n\n\nâŒ 7. Not scalable\n\nToday: 2 PDFsTomorrow: 20â€“200 documents\n\nchrome.storage will fail.\n\nYour AI features (RAG, system prompt, autofill) need a reliable document store.\n\n\n\nâŒ 8. Not accessible by backend / cloud\n\nchrome.storage is local to the browser.\n\nYour backend MCP server cannot fetch documents stored inside chrome.storage.\n\nBut your autofill workflow needs:\n\nBrowser Extension â†’ Backend (MCP) â†’ AI Model â†’ Playwright â†’ Autofill\n\nBack-end cannot access chrome.storage â†’ breaks workflo\n\n\n\n",
        "detectedPatterns": [
          "key-value"
        ],
        "keyValuePairs": {
          "Today": "2 PDFsTomorrow: 20â€“200 documents"
        }
      }
    },
    {
      "filename": "AI Model cost.docx",
      "fileType": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
      "extension": "docx",
      "size": 22814,
      "lastModified": "2025-11-18T10:06:20.691Z",
      "extractedAt": "2025-11-27T10:45:07.098Z",
      "data": {
        "type": "unstructured_text",
        "lineCount": 345,
        "content": "AI Model\n\nPrice per 1M tokens (USD)\n\nPrice for 1 form (1000 tokens)\n\nPrice in INR\n\nForms possible for â‚¹100\n\nGPT-4o-mini\n\n$0.75\n\n$0.00075\n\nâ‚¹0.065\n\n~1500 forms\n\nGemini Flash 2.0\n\n$0.30\n\n$0.00030\n\nâ‚¹0.026\n\n~3800 forms\n\nClaude 3.5 Haiku\n\n$1.50\n\n$0.00150\n\nâ‚¹0.13\n\n~760 forms\n\nLlama 3.1 8B (local)\n\nFREE\n\nFREE\n\nFREE\n\nUnlimited\n\nGPT-4o (full)\n\n$5.00\n\n$0.005\n\nâ‚¹0.43\n\n~230 forms\n\nClaude Opus\n\n$15.00\n\n$0.015\n\nâ‚¹1.30\n\n~75 forms\n\nGPT-4o-mini\n\nCan handle entire pipeline alone\n\nMinimal cost\n\nBest mapping accuracy\n\nBest for real-world autofill\n\nStable output\n\nCost per form: ~â‚¹0.065Forms per â‚¹100: ~1500Daily possible:\n\nâ‚¹100/day âž 1500 forms\n\nâ‚¹500/day âž 7500 forms\n\nâ‚¹1000/day âž 15,000 forms\n\n\n\nðŸ¥ˆ Cheapest Option (for large scale)\n\nGemini Flash 2.0\n\n60% cheaper than GPT-4o-mini\n\nVery fast\n\nGood classification\n\nMapping slightly weaker\n\nBest for high volume low-complexity forms\n\nCost per form: ~â‚¹0.026Forms per â‚¹100: ~3800Daily possible:\n\nâ‚¹100/day âž 3800 forms\n\nâ‚¹500/day âž 19,000 forms\n\nâ‚¹1000/day âž 38,000 forms\n\n\n\nðŸ¥‰ Zero Cost Option\n\nLlama 3.1 8B (local)\n\nFREE\n\nNo token cost\n\nGood for basic mapping\n\nNot reliable for complex forms\n\nNeeds GPU for speed\n\nCost per form: â‚¹0Forms per â‚¹100: Unlimited\n\n\n\nðŸ”¥ FINAL RANKING (Best â†’ OK)\n\nRank\n\nAI\n\nWhy\n\nâ­ #1\n\nGPT-4o-mini\n\nBest accuracy + low cost + handles everything\n\nâ­ #2\n\nGemini Flash 2.0\n\nCheapest + fast, but mapping is weaker\n\nâ­ #3\n\nClaude Haiku\n\nGood, but expensive compared to GPT-4o-mini\n\nâ­ #4\n\nLlama 3.1 8B\n\nFree but weak for complex mapping\n\nâŒ #5\n\nGPT-4o full\n\nToo expensive for your use case\n\nâŒ #6\n\nClaude Opus\n\nOverkill + expensive\n\n\n\nðŸ§  Recommended for Your Project\n\nUse one AI for everything:\n\nGPT-4o-mini\n\nWhy:\n\nHandles all steps perfectly\n\nGood at field meaning extraction\n\nUnderstands abbreviations\n\nGreat with JSON\n\nSupports RAG context\n\nCost extremely low\n\nStable and predictable\n\n\n\nðŸŽ¯ SUMMARY IN ONE LINE\n\nUse GPT-4o-mini as your single common AI â†’ Cost ~â‚¹0.065 per form â†’ You can process ~1500 forms for only â‚¹100.\n\nAI Daily Cost Table (In Indian Rupees â‚¹)\n\n1ï¸âƒ£ GPT-4o-mini (BEST overall)\n\nCost 1M tokens = $0.75 = â‚¹64.5\n\nDaily Requests\n\nTokens/day\n\nCost/day (â‚¹)\n\n100 req/day\n\n100k\n\nâ‚¹6.45\n\n500 req/day\n\n500k\n\nâ‚¹32.25\n\n1000 req/day\n\n1M\n\nâ‚¹64.5\n\n5000 req/day\n\n5M\n\nâ‚¹322\n\n10,000 req/day\n\n10M\n\nâ‚¹645\n\nðŸ‘‰ Very cheap. Very accurate. Recommended.\n\n\n\n2ï¸âƒ£ Gemini Flash 2.0 (Cheapest)\n\nCost 1M tokens = $0.30 = â‚¹25.8\n\nDaily Requests\n\nTokens/day\n\nCost/day (â‚¹)\n\n100 req/day\n\n100k\n\nâ‚¹2.58\n\n500 req/day\n\n500k\n\nâ‚¹12.9\n\n1000 req/day\n\n1M\n\nâ‚¹25.8\n\n5000 req/day\n\n5M\n\nâ‚¹129\n\n10,000 req/day\n\n10M\n\nâ‚¹258\n\nðŸ‘‰ Ultra cheap but mapping accuracy slightly lower than GPT-4o-mini.\n\n\n\n3ï¸âƒ£ Claude 3.5 Haiku\n\nCost 1M tokens = $1.50 = â‚¹129\n\nDaily Requests\n\nTokens/day\n\nCost/day (â‚¹)\n\n100 req/day\n\n100k\n\nâ‚¹12.9\n\n500 req/day\n\n500k\n\nâ‚¹64.5\n\n1000 req/day\n\n1M\n\nâ‚¹129\n\n5000 req/day\n\n5M\n\nâ‚¹645\n\n10,000 req/day\n\n10M\n\nâ‚¹1290\n\nðŸ‘‰ More expensive, accuracy good but not better than GPT-4o-mini.\n\n\n\n4ï¸âƒ£ Llama 3.1 (local model â€” FREE)\n\nCost/day = â‚¹0Requests/day = Unlimited(but accuracy is low for complex form mapping)\n\nDaily Requests\n\nCost/day\n\n10,000\n\nâ‚¹0\n\n50,000\n\nâ‚¹0\n\n100,000\n\nâ‚¹0\n\n1,000,000\n\nâ‚¹0\n\nðŸ‘‰ Good if you want zero cost, but not reliable.\n\n.\n\n\n\nðŸ§  ASSUMPTION FOR MONTHLY COST\n\nEach request = 1000 tokens(Your form detection + mapping + RAG formatting fits in 800â€“1200 tokens)\n\nMonthly usage levels:\n\nTier\n\nRequests per Month\n\nTier 1\n\n10,000 requests\n\nTier 2\n\n50,000 requests\n\nTier 3\n\n100,000 requests\n\nTier 4\n\n500,000 requests\n\nTier 5\n\n1,000,000 requests\n\n\n\nðŸ’° 1M TOKEN COST (IN INR)\n\nAI Model\n\n1M token cost (USD)\n\nCost in INR\n\nGemini Flash 2.0\n\n$0.30\n\nâ‚¹26\n\nGPT-4o-mini\n\n$0.75\n\nâ‚¹64.5\n\nClaude Haiku\n\n$1.50\n\nâ‚¹129\n\nGPT-4o (main)\n\n$5.00\n\nâ‚¹430\n\nClaude Opus\n\n$15.00\n\nâ‚¹1290\n\nLlama local\n\nFREE\n\nFREE\n\n\n\nðŸ“… MONTHLY COST TABLE (Very Easy)\n\nFormula:\n\nMonthly Cost = Requests Ã— â‚¹0.065 (GPT-4o-mini)\n\nMonthly Cost = Requests Ã— â‚¹0.026 (Gemini Flash)\n\n\n\nðŸ§® GPT-4o-mini â€” Monthly Cost (Recommended AI)\n\nMonthly Requests\n\nMonthly Tokens\n\nCost (INR)\n\n10,000\n\n10M tokens\n\nâ‚¹645\n\n50,000\n\n50M tokens\n\nâ‚¹3225\n\n100,000\n\n100M tokens\n\nâ‚¹6450\n\n500,000\n\n500M tokens\n\nâ‚¹32,250\n\n1,000,000\n\n1B tokens\n\nâ‚¹64,500\n\nâ­ Best If:\n\nYou want accuracy\n\nMapping must be correct\n\nForms are unpredictable\n\nYou want stable JSON outputs\n\nYou have real customers\n\n\n\nðŸ§® Gemini Flash 2.0 â€” Cheapest Monthly Cost\n\nMonthly Requests\n\nMonthly Tokens\n\nCost (INR)\n\n10,000\n\n10M\n\nâ‚¹260\n\n50,000\n\n50M\n\nâ‚¹1290\n\n100,000\n\n100M\n\nâ‚¹2580\n\n500,000\n\n500M\n\nâ‚¹12,900\n\n1,000,000\n\n1B\n\nâ‚¹25,800\n\nâ­ Best If:\n\nYou want MINIMUM cost\n\nForms are simple\n\nMapping accuracy is not mission critical\n\n\n\nðŸ§® Claude Haiku â€” Monthly Cost\n\nMonthly Requests\n\nCost (INR)\n\n10k\n\nâ‚¹1290\n\n50k\n\nâ‚¹6450\n\n100k\n\nâ‚¹12,900\n\n500k\n\nâ‚¹64,500\n\n1M\n\nâ‚¹129,000\n\nâ­ Best If:\n\nYou need safety\n\nBusiness/legal forms\n\nNot cost sensitive\n\n\n\nðŸ§® Llama Local â€” Monthly Cost\n\nRequests\n\nCost\n\nUnlimited\n\nâ‚¹0\n\nâ­ Best If:\n\nYou need ZERO cost\n\nOkay with lower accuracy\n\nHave CPU/GPU server\n\n\n\nðŸ† UPGRADE PLAN â€” Which AI at Which Stage of Your Startup\n\nStage 1: Prototype (0â€“10k requests per month)\n\nUse:\n\nâœ” Llama 3.1 Local OR Gemini Flash 2.0\n\nCost:\n\nFree (local)\n\nOr â‚¹260/month (Gemini)\n\nReason:\n\nLow traffic\n\nBudget saving\n\nGood enough for early tests\n\n\n\nStage 2: MVP / Early Users (10kâ€“100k requests per month)\n\nSwitch to:\n\nâ­ GPT-4o-mini\n\nCost:\n\nâ‚¹645 to â‚¹6450 per month\n\nReason:\n\nHigher mapping accuracy\n\nBetter autofill logic\n\nStable JSON outputs\n\nCan handle messy forms\n\nNeeded before production launch\n\n\n\nStage 3: Production (100kâ€“500k requests per month)\n\nUse:\n\nâ­ GPT-4o-mini\n\nOR\n\nâ­ Gemini Flash + GPT-4o-mini hybrid\n\nCost:\n\nâ‚¹6450 to â‚¹32,000\n\nFlow:\n\nGemini Flash â†’ cheap classification\n\nGPT-4o-mini â†’ mapping + reasoning (core)\n\nThis reduces cost by 40â€“60%.\n\n\n\nStage 4: Enterprise Scale (500kâ€“1M+ requests per month)\n\nMix:\n\nâœ” GPT-4o-mini for mapping\n\nâœ” Gemini Flash for basic steps\n\nâœ” Local Llama for caching repeated forms\n\nCost:\n\nâ‚¹32k to â‚¹1L per month(based on traffic)\n\n\n\nðŸ§  FINAL RECOMMENDATION (For Your Project)\n\nUse GPT-4o-mini as the main AI\n\nbecause:\n\nBest mapping accuracy\n\nBest for complex forms\n\nLowest cost for high intelligence\n\nPerfect for your MCP + Playwright flow\n\nWorks with RAG\n\nExpected monthly cost:\n\nFor 10,000 requests â†’ â‚¹645\n\nFor 50,000 requests â†’ â‚¹3,225\n\nFor 100,000 requests â†’ â‚¹6,450\n\nExtremely affordable.\n\n\n\n",
        "detectedPatterns": [
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value"
        ],
        "keyValuePairs": {
          "Cost per form": "â‚¹0Forms per â‚¹100: Unlimited",
          "Stage 1": "Prototype (0â€“10k requests per month)",
          "Stage 2": "MVP / Early Users (10kâ€“100k requests per month)",
          "Stage 3": "Production (100kâ€“500k requests per month)",
          "Stage 4": "Enterprise Scale (500kâ€“1M+ requests per month)"
        }
      }
    },
    {
      "filename": "AI_n8n_vectordatabase.docx",
      "fileType": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
      "extension": "docx",
      "size": 2381769,
      "lastModified": "2025-10-15T12:35:06.390Z",
      "extractedAt": "2025-11-27T10:45:07.430Z",
      "data": {
        "type": "unstructured_text",
        "lineCount": 364,
        "content": "Generative AI Concepts\n\n1. Natural Language Processing (NLP)\n\nOverview\n\nNatural Language Processing (NLP) is a field of AI that enables machines to understand, interpret, and generate human language.Itâ€™s the foundation for most Generative AI applications â€” from chatbots to document summarization.\n\n\n\nA. Text Processing\n\nBefore any model can process text, raw data must be cleaned and standardized.\n\nSteps in Text Preprocessing\n\nLowercasingConverts all characters to lowercase to avoid treating words like â€œChatGPTâ€ and â€œchatgptâ€ differently.\n\ntext = text.lower()\n\nRemoving PunctuationRemoves characters like !, ,, . which carry less meaning for models.\n\nimport string\n\ntext = text.translate(str.maketrans('', '', string.punctuation))\n\nRemoving StopwordsEliminates common words such as the, is, and, a that donâ€™t add semantic value.\n\nfrom nltk.corpus import stopwords\n\nwords = [w for w in text.split() if w not in stopwords.words('english')]\n\nStemmingReduces words to their root form â€” e.g., running â†’ run (uses heuristics).\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n\nstemmed = [stemmer.stem(word) for word in words]\n\nLemmatizationConverts words to their dictionary form â€” e.g., better â†’ good (uses vocabulary + context).\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nlemmatized = [lemmatizer.lemmatize(word) for word in words]\n\n\n\nB. Tokenization\n\nTokenization breaks text into smaller units (tokens) such as words, subwords, or sentences.\n\nTypes of Tokenizers\n\nType\n\nExample\n\nUsed In\n\nWord Tokenizer\n\n[\"I\", \"love\", \"AI\"]\n\nSimple models\n\nSubword Tokenizer\n\n[\"play\", \"##ing\"]\n\nTransformers (BERT, GPT)\n\nSentence Tokenizer\n\n[\"I love AI.\", \"Itâ€™s powerful.\"]\n\nSummarization, translation\n\nTokenization prepares text for embedding generation and model input.\n\n\n\nC. Embeddings\n\nEmbeddings convert words or sentences into numerical vectors that capture meaning.Words with similar meanings will have similar vector representations.\n\n Workflow of Embedding Generation\n\nInput text â†’ Tokenization\n\nTokens â†’ Converted to numeric IDs\n\nEach ID â†’ Mapped to an embedding vector (dense representation)\n\nThese vectors represent semantic meaning in continuous space.\n\nExample:\n\nking - man + woman â‰ˆ queen\n\n Common Embedding Models\n\nWord2Vec\n\nGloVe\n\nFastText\n\nBERT embeddings\n\nSentence Transformers\n\n\n\n2. Attention Mechanism\n\nDefinition\n\nThe Attention Mechanism helps models â€œfocusâ€ on the most relevant parts of input data when generating or understanding language.\n\nInstead of treating all words equally, attention assigns scores (weights) to highlight important words.\n\n\n\n How It Works\n\nStep 1ï¸: Calculate Attention Scores\n\nFor each word in a sentence:\n\nCompute how much attention it should give to other words.\n\nUse dot-product similarity between query and key vectors.\n\nFormula:[\\text{score}(Q, K) = Q \\cdot K^T]\n\nStep 2ï¸: Normalize Scores\n\nApply Softmax to get probabilities:[\\alpha_i = \\frac{e^{score_i}}{\\sum_j e^{score_j}}]\n\nStep 3ï¸  :  Adjust Focus Based on Scores\n\nMultiply the scores with value vectors to generate the context vector:[\\text{Attention}(Q,K,V) = \\text{Softmax}(QK^T)V]\n\nThis gives a weighted representation of information most relevant to the current word.\n\n\n\n3. Transformers\n\nDefinition\n\nTransformers are deep learning architectures that rely entirely on attention mechanisms (no recurrence or convolution).They are the backbone of models like BERT, GPT, T5, etc.\n\n\n\n\n\n\n\n\n\n\n\nComparison: Older vs Newer NLP Models\n\nModel Type\n\nExamples\n\nLimitations\n\nTransformers Fix\n\nTraditional (RNN, LSTM, GRU)\n\nSeq2Seq, LSTM\n\nSequential â†’ slow, hard to parallelize\n\nParallelized processing\n\nTransformers\n\nBERT, GPT\n\nUse self-attention â†’ process entire context simultaneously\n\nBetter context understanding, faster training\n\n\n\nHow Transformers Work (Step-by-Step)\n\nInput Tokenization â†’ Convert text to tokens\n\nEmbedding Layer â†’ Each token â†’ embedding vector\n\nPositional Encoding â†’ Adds order information (since transformer has no sequence memory)\n\nSelf-Attention Layer â†’ Calculates relationships between all tokens\n\nFeed Forward Network â†’ Processes each token independently\n\nNormalization + Residual Connections â†’ Stability and gradient flow\n\nStack Multiple Layers â†’ Deep understanding of context\n\nOutput Layer â†’ Generates or classifies text\n\n\n\n4. Text Similarity\n\nText similarity measures how close two pieces of text are in meaning.\n\nMethod\n\nDescription\n\nFormula / Idea\n\nCosine Similarity\n\nMeasures angle between two vectors\n\n(\\cos(\\theta) = \\frac{A \\cdot B}{\n\nJaccard Similarity\n\nMeasures overlap between sets\n\n(J(A,B) = \\frac{\n\nEuclidean Distance\n\nMeasures straight-line distance between vectors\n\n(d(A,B) = \\sqrt{\\sum (A_i - B_i)^2})\n\n\n\n5. Information Retrieval (IR)\n\nIR focuses on finding relevant documents from a large collection.\n\nðŸ”¹ Components\n\nDocument Representation\n\nConvert text into vectors using TF-IDF, BM25, or embeddings.\n\nIndexing\n\nStore document vectors in searchable structure.\n\nScoring & Ranking\n\nCalculate similarity between query and document.\n\nRank documents based on relevance score.\n\n\n\n6. Introduction to RAG (Retrieval-Augmented Generation)\n\nRAG combines retrieval (finding facts) and generation (creating natural language output).\n\nWorkflow:\n\nRetrieve relevant information from external sources.\n\nAugment the prompt with retrieved data.\n\nGenerate coherent responses using a generative model (like GPT).\n\nâœ… This improves factual accuracy and reduces hallucination.\n\n\n\n7. Retrieval Models\n\n Definition\n\nRetrieval models find the most relevant documents for a query.\n\nTypes\n\nTraditional (Sparse)\n\nDense (Neural)\n\n\n\n Traditional Models\n\n(a) TF-IDF\n\nRepresents text as word frequencies.\n\nAssigns higher weight to words that appear frequently in a document but rarely in others.\n\nLimitations:\n\nIgnores meaning/synonyms.\n\nSparse, large vectors.\n\n\n\n(b) BM25\n\nAn improved TF-IDF variant.\n\nConsiders term frequency saturation and document length normalization.\n\nBenefits:\n\nMore accurate for keyword-based search.\n\nStrong baseline in IR systems.\n\nLimitations:\n\nStill purely lexical â€” canâ€™t understand semantics.\n\n\n\nDense Retrieval Models\n\nUse neural networks to represent documents and queries in a shared embedding space.\n\nWorkflow:\n\nEncode query â†’ embedding\n\nEncode documents â†’ embeddings\n\nCompute cosine similarity\n\nRetrieve top-k most similar documents\n\nâœ… Benefits:\n\nUnderstand semantic meaning\n\nSupports contextual queries\n\nâš ï¸ Limitations:\n\nComputationally heavy\n\nRequires large training data\n\n\n\nAbsolutely ðŸ‘ â€” hereâ€™s a concise, clear version of the DPR (Dense Passage Retrieval) workflow, perfect for your documentation:\n\n\n\nDPR (Dense Passage Retrieval)\n\nDefinition:DPR is a neural retrieval model that finds semantically relevant passages for a given query using dense embeddings instead of keyword matching.\n\n\n\nWorkflow Steps\n\nInput:\n\nTakes a query (e.g., â€œWho founded PostgreSQL?â€)\n\nAnd a collection of passages/documents\n\nEncoders:\n\nQuery Encoder â†’ converts query into a dense vector\n\nPassage Encoder â†’ converts each passage into a dense vector\n\nBoth are typically BERT-based and map text into the same vector space.\n\nCompute Similarity:\n\nCalculate cosine similarity between query and passage vectors\n\nHigher score = higher semantic relevance\n\nRank Documents:\n\nRank all passages based on similarity scores\n\nSelect the Top-K most relevant ones\n\nOutput:\n\nReturns top passages as retrieved context for RAG or QA systems\n\nThese passages are used by the generator (like GPT) to produce final, factual responses.\n\n\n\n8. Generative Models\n\nDefinition\n\nGenerative models produce new text by predicting the next token based on previous context (e.g., GPT, T5, LLaMA).\n\nAdvantages over Retrieval Models\n\nRetrieval Models\n\nGenerative Models\n\nFetch existing data\n\nGenerate new data\n\nKeyword or embedding-based\n\nContext and pattern-based\n\nStatic knowledge\n\nCan generalize & reason\n\nLimited creativity\n\nHighly flexible & adaptive\n\n\n\n Workflow of Generative Models\n\nTokenize input text\n\nPass through transformer layers\n\nCompute attention & hidden states\n\nPredict next token (autoregressive generation)\n\nRepeat until complete output\n\n\n\nComparison: Retrieval vs Generative\n\nFeature\n\nRetrieval\n\nGenerative\n\nOutput\n\nFactual text from memory\n\nNew content\n\nContext\n\nNarrow\n\nDeep contextual\n\nExample\n\nSearch engine\n\nChatGPT\n\nLimitation\n\nCanâ€™t create new info\n\nMay hallucinate\n\nBest Use\n\nInformation lookup\n\nConversation, summarization\n\n\n\n\n\n\n\n9. Combining Retrieval + Generation (RAG Systems)\n\n Why Combine?\n\nBecause:\n\nRetrieval ensures accuracy & factual grounding\n\nGeneration ensures natural, coherent language\n\n\n\nSteps in Retrieval\n\nRepresent query\n\nSearch index\n\nScore and rank results\n\nReturn top relevant documents\n\nSteps in Generation\n\nTake retrieved documents\n\nAppend to user query (context window)\n\nGenerate text response using LLM\n\nRAG = Retrieval + Generation\n\nQuery â†’ Retrieve supporting facts\n\nFused prompt â†’ Pass to LLM\n\nLLM â†’ Generate answer with evidence\n\nâœ… Result: Factual, context-aware, and fluent outputs.\n\n\n\nN8N:\n\nn8n is a workflow automation tool that lets  connect apps and services without writing complex code. Itâ€™s like a visual builder for automating tasks â€” perfect for NGOs, SHGs, and small teams.\n\nUse Case\n\nWhat n8n Can Automate\n\nGrant Alerts\n\nFetch new grants from websites or emails and send updates to WhatsApp, Telegram, or email\n\nEmail Parsing\n\nExtract keywords, deadlines, and links from incoming emails\n\nTranslation\n\nAutomatically translate alerts into Tamil using AI or APIs\n\nVoice Playback\n\nConvert grant summaries into audio using text-to-speech tools\n\nDashboard Sync\n\nPush data into Google Sheets, Airtable, or Streamlit dashboards\n\nReminders\n\nSchedule follow-ups or deadline alerts for field teams\n\nForm Collection\n\nCollect responses from Typeform, Google Forms, or WhatsApp and store them in a database\n\nï‚·  Nodes: Each step in your workflow (e.g., \"Get email\", \"Translate text\", \"Send WhatsApp\")\n\nï‚·  Triggers: Start the workflow (e.g., \"New email received\", \"Every morning at 9 AM\")\n\nï‚·  Credentials: Securely connect to services like Gmail, Telegram, Airtable, etc.\n\nï‚·  Variables: Pass data between steps (e.g., grant title, deadline, link)\n\nProject 1:\n\nThis project automatically handles form submissions and organizes responses. When someone fills out a formâ€”such as a grant interest or contact formâ€”the workflow starts immediately. It collects the submitted details like name, organization, and message. Then, it sends a personalized email reply to confirm receipt and thank the person. At the same time, the form data is added to a Google Sheet, so your team can track and follow up easily. This saves time, avoids manual work, and keeps everything organized.\n\nWorkflow Overview\n\nTrigger â†’ Form Submission â†’ Email Response â†’ Google Sheets Entry\n\n\n\n\n\n\n\n\n\n\n\nProject 2: \n\nAutomated Workflow in n8n for Scheduled Data Retrieval, Filtering, Alerts, and Google Sheet Logging\n\nSchedule Trigger The workflow starts automatically at a set time â€” for example, every morning or once a week.\n\nHTTP Request It sends a GET request to an external API (https://appapi.com/space) to fetch data â€” maybe space news, updates, or satellite info.\n\nPython Code (Beta) Custom Python code is used to process or clean the data â€” like extracting key fields, formatting text, or filtering results.\n\nEdit Fields This step lets you manually adjust or enrich the data â€” for example, adding a label, correcting a value, or tagging it for a specific use.\n\nSend a Message (Gmail) The processed data is sent as an email â€” maybe to notify a team, share a summary, or alert someone about new info.\n\nAppend Row in Google Sheets Finally, the data is saved in a Google Sheet â€” creating a permanent record that can be tracked, filtered, or visualized later.\n\n\n\n\n\n\n\n\n\nProject 3\n\nThis workflow is designed to automatically process files uploaded to Google Drive and make them searchable through an AI-powered chat system. It begins when a new file is created in a specific Google Drive folder. The system immediately downloads the file and stores it in a local folder. Then, it reads the content of the file and converts it into embeddings using Ollama â€” a model that transforms text into a format that can be searched semantically. These embeddings are saved in a vector store, which acts like a smart database that understands meaning, not just keywords. When a user sends a message or asks a question through the chat interface, the AI agent is triggered. It uses the stored embeddings to find the most relevant information from the uploaded files. The agent then replies with a clear, context-aware answer based on the content. This setup allows users to ask questions about any document in the folder and get instant, intelligent responses â€” perfect for grant tracking, report analysis, or multilingual document support.\n\n\n\nVector Database:\n\nA vector database is specifically designed to store and search high-dimensional vectors â€” like word embeddings, image embeddings, or audio embeddings.\n\n                         \n\nTraditional Database:\n\nMost real-world data is unstructured, and storing it in traditional databases like MySQL, PostgreSQL, or SQLite comes with limitations.\n\nUnstructured data includes:\n\n Images\n\nAudio files\n\nPDFs and scanned documents\n\nFree-form text (emails, notes, social media posts)\n\nIt doesnâ€™t fit neatly into rows and columns like structured data (e.g., names, dates, numbers).\n\nVector Embeddings:\n\nA vector embedding is:A numerical fingerprint of something unstructured.Instead of storing raw text like \"school grant for rural girls\", we convert it into a vector like:\n\n[0.12, -0.45, 0.88, ..., 0.03]  â† (e.g., 384 or 768 numbers)\n\n\n\n\n\nVector embeddings are a way to turn complex data like text, images, or audio into a list of numbers that capture their meaning. Instead of just storing raw words or files, we use AI models to convert them into these numerical representations, which are called embeddings. These embeddings help us compare and search data based on similarity â€” for example, finding documents that talk about the same topic even if they use different words. Once we have these embeddings, we store them in a special type of database called a vector database, which is designed to handle high-dimensional data and perform fast, meaning-based searches. This is especially useful for building smart applications like grant search tools, chatbots, or recommendation systems that need to understand and retrieve information based on context, not just exact matches.\n\nHow Embedding Model Work:\n\n\n\n\n\n\n\nHow Word Embeddings Are Created and Used\n\nLearning from Language AI models (like BERT or OpenAIâ€™s embedding models) read millions of sentences and learn how words relate to each other â€” like how â€œkingâ€ often appears near â€œroyalâ€ or â€œpower.â€\n\nTurning Words into Numbers Each word is converted into a vector â€” a long list of numbers (like [0.12, -0.45, 0.88, ...]) that captures its meaning, context, and relationships.\n\nPlacing Words in Space These vectors are plotted in a multi-dimensional space. Words with similar meanings (like â€œkingâ€ and â€œqueenâ€) end up close together. Words with different meanings (like â€œmonkeyâ€ and â€œqueenâ€) are far apart.\n\nUsing Embeddings for Smart Search When someone types a query like â€œfunds for rural schools,â€ the system converts it into a vector and finds documents or grants with similar vectors â€” even if they donâ€™t use the exact same words.\n\nSome Embedding Models:\n\nOpen Ai embedding\n\nGPT\n\nPAID\n\nHugging Face Embedding\n\nOpen LLm\n\nFREE\n\nLlama2 Embedding\n\nFacebook\n\nFREE\n\nGoogle Palm Embedding\n\nGoogle\n\nFREE\n\n\n\nExample:\n\n\n\n\n\n\n\nWhy Searching with Euclidean or Cosine Distance Is Slow\n\nWhen we want to find similar vectors (e.g., similar grant descriptions),  usually:\n\nUse Euclidean distance (how far apart two vectors are)\n\nOr Cosine similarity (how aligned two vectors are)\n\nBut if we have 10,000 or 1 million vectors, the system must:\n\nCompare query vector to every single stored vector\n\nCalculate the distance for each one\n\nSort and return the closest matches\n\nThis is called brute-force search â€” and itâ€™s slow for large datasets.\n\nVector Index:\n\n\n\nA vector index is a smart structure that organizes vectors so you donâ€™t have to compare all of them.\n\nIt uses techniques like:\n\nClustering (grouping similar vectors together)\n\nGraphs (like HNSW: Hierarchical Navigable Small World)\n\nHashing (like LSH: Locality Sensitive Hashing)\n\n\n\n\n\nUseCases of Vector Database:\n\nLong-Term Memory for LLMs\n\nLarge Language Models (LLMs) like PaLM or GPT canâ€™t remember past conversations or documents unless we give them that context again. Vector DBs act like memory â€” storing embeddings of previous chats, documents, or user actions. When the model needs context, it retrieves relevant info using semantic similarity.\n\nExample: Your grant assistant remembers past queries like â€œeducation funds in Tamil Naduâ€ and retrieves similar grants automatically.\n\nSemantic Search\n\nInstead of matching exact keywords, semantic search finds results based on meaning. Vector DBs store text as embeddings, so you can search for â€œfunds for rural girlsâ€ and still find documents that say â€œsupport for female students in villages.â€\n\nExample: A user types in Tamil or English, and your dashboard shows relevant grants even if the words donâ€™t match exactly.\n\nSimilarity Search (Text, Images, Videos, Audios)\n\nVector DBs can store embeddings for any media type â€” not just text. You can search for similar images, voice notes, or videos by comparing their vector representations.\n\nExample: Upload a flyer or voice note, and the system finds similar grants or alerts.\n\nRecommendation Engine\n\nBy comparing user queries or behavior to stored embeddings, vector DBs can suggest related content. This powers smart recommendations â€” like â€œwe might also be interested inâ€¦â€\n\nExample: After viewing a health grant, the system recommends nearby sanitation or nutrition programs.\n\nSome Vector Database:\n\nChroma\n\nFAISS\n\nWeaviate\n\nPinecone\n\nRedis\n\nMilvus\n\nQdrant\n\nVespa\n\nVald\n\nZilliz\n\nChroma\n\nChroma is the open-source AI application database. Chroma makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs\n\nInstall chroma:\n\npip install chromadb\n\nDownload articles:\n\nInvoke-WebRequest -Uri \"https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\" -OutFile \"new_articles.zip\"\n\nUnzip file:\n\nExpand-Archive -Path \"new_articles.zip\" -DestinationPath \"new_articles\"\n\nWhat is Chunking?\n\nChunking means splitting a large document into smaller pieces called chunks. These chunks are easier for AI models to understand, search, and answer questions from.\n\nWhat is Chunk Size?\n\nChunk size is the maximum number of characters allowed in each chunk.\n\nFor example:\n\nIf chunk size is set to 1000, each chunk will contain up to 1000 characters from the original text.\n\nThink of it like cutting a long rope into smaller sectionsâ€”each section is a chunk.\n\nWhat is Chunk Overlap?\n\nChunk overlap means that the end of one chunk is repeated at the beginning of the next chunk.\n\nThis helps the AI maintain context between chunks. Without overlap, important connections between sentences might be lost.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "detectedPatterns": [
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value"
        ],
        "keyValuePairs": {
          "Step 1ï¸": "Calculate Attention Scores",
          "Formula": "[\\text{score}(Q, K) = Q \\cdot K^T]",
          "Step 2ï¸": "Normalize Scores",
          "Apply Softmax to get probabilities": "[\\alpha_i = \\frac{e^{score_i}}{\\sum_j e^{score_j}}]",
          "Step 3ï¸": "Adjust Focus Based on Scores",
          "Multiply the scores with value vectors to generate the context vector": "[\\text{Attention}(Q,K,V) = \\text{Softmax}(QK^T)V]",
          "Comparison": "Retrieval vs Generative",
          "Definition": "DPR is a neural retrieval model that finds semantically relevant passages for a given query using dense embeddings instead of keyword matching.",
          "âœ… Result": "Factual, context-aware, and fluent outputs.",
          "ï‚·  Nodes": "Each step in your workflow (e.g., \"Get email\", \"Translate text\", \"Send WhatsApp\")",
          "ï‚·  Triggers": "Start the workflow (e.g., \"New email received\", \"Every morning at 9 AM\")",
          "ï‚·  Credentials": "Securely connect to services like Gmail, Telegram, Airtable, etc.",
          "ï‚·  Variables": "Pass data between steps (e.g., grant title, deadline, link)",
          "Project 2": "",
          "HTTP Request It sends a GET request to an external API (https": "//appapi.com/space) to fetch data â€” maybe space news, updates, or satellite info.",
          "A vector embedding is": "A numerical fingerprint of something unstructured.Instead of storing raw text like \"school grant for rural girls\", we convert it into a vector like:",
          "Graphs (like HNSW": "Hierarchical Navigable Small World)",
          "Hashing (like LSH": "Locality Sensitive Hashing)",
          "Example": "After viewing a health grant, the system recommends nearby sanitation or nutrition programs.",
          "Invoke-WebRequest -Uri \"https": "//www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\" -OutFile \"new_articles.zip\""
        }
      }
    },
    {
      "filename": "Ai_PYTHON.docx",
      "fileType": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
      "extension": "docx",
      "size": 635514,
      "lastModified": "2025-09-25T05:40:09.284Z",
      "extractedAt": "2025-11-27T10:45:07.915Z",
      "data": {
        "type": "unstructured_text",
        "lineCount": 909,
        "content": "\n\nCourse Name: AI Python for Beginners\n\nIntroduction to Python\n\nPython is one of the most popular programming languages in the world. Itâ€™s known for:\n\nA large and supportive developer community\n\nBeing used in many fields, including:\n\nSelf-driving cars\n\nChatbots\n\nSmart agriculture\n\nApplications and software development\n\nPython is also widely used in AI, making it ideal for projects like the ones youâ€™re building.\n\n\n\nData in Python\n\nStrings\n\nStrings are used to store text. Python supports:\n\nSingle-line strings â†’ Enclosed in single (') or double (\") quotes\n\nMulti-line strings â†’ Enclosed in triple single (''') or triple double (\"\"\") quotes\n\nMain difference:\n\nSingle-line string â†’ Written in one line\n\nMulti-line string â†’ Written across multiple lines\n\nExample:\n\nsingle_line = \"Hello, Python!\"\n\nmulti_line = \"\"\"This is a \n\nmulti-line string\n\nin Python.\"\"\"\n\n\n\nNumbers\n\nPython supports different types of numbers:\n\nIntegers â†’ Whole numbers, e.g., 5, -3\n\nFloats â†’ Decimal numbers, e.g., 3.14, -0.001\n\nBooleans â†’ True/False values, e.g., True, False\n\n\n\nBasic Functions for Data\n\ntype() â†’ Tells the type of data\n\nprint() â†’ Displays value/output on screen\n\n\n\nF-Strings (Formatted Strings)\n\nF-strings let you insert variables or expressions directly into a string using {}.\n\nExamples:\n\na = 10\n\nb = 20\n\nprint(f\"The addition of a + b is {a+b}\")  # Output: The addition of a + b is 30\n\n\n\npi = 3.14159\n\nnum = 12345\n\nprint(f\"Pi: {pi:.2f}\")   # Pi: 3.14\n\nprint(f\"Number: {num:,}\") # Number: 12,345\n\nDebugging tip:\n\nage = 30\n\nprint(f\"{age=}\")  # Output: age=30\n\nHere, {variable=} automatically prints the variable name and its current value.\n\n\n\nVariables\n\nVariables are used to store data so you can reuse it later.\n\n\n\nLLM-LLM stands for Large Language Model â€” a type of artificial intelligence (AI) model trained to understand and generate human-like language\n\nExamples: \n\nOpenAI GPT-3.5 / GPT-4\n\nMeta's LLaMA\n\nGoogle's PaLM\n\nhow to use a Large Language Model (LLM) in a Python program.\n\n\n\nOpenAIâ€™s GPT-4o-mini model\n\nA .env file to keep your secret API key safe\n\nimport os\n\nimporting a built-in Python library called os.\n\nPython code talk to  computerâ€™s environment (like reading secret keys stored in environment variables).\n\nfrom dotenv import load_dotenv\n\ntool called dotenv to load a .env file\n\nThis .env file contains secret info like your API key:\n\nOPENAI_API_KEY=your-secret-key-here It's a safe way to store sensitive info outside your main code.\n\n\n\nload_dotenv('.env', override=True)\n\nLoads the .env file.\n\noverride=True means: If there's already an environment variable, replace it with the one in .env.\n\n\n\nopenai_api_key = os.getenv('OPENAI_API_KEY')\n\ngetting the API key from the environment and saving it in a variable called openai_api_key.\n\nThis is the key that lets  code use OpenAI's servers.\n\nclient = OpenAI(api_key=openai_api_key)\n\nThis creates a client object to talk to OpenAI's services (like ChatGPT).\n\nclient to send messages and get responses from the model.\n\ndef get_llm_response(prompt: str) -> str:\n\nThis is a function. call it when  want a response from the AI.\n\npass in a prompt (your question or message), and it returns a response from the model.\n\ncompletion = client.chat.completions.create(...)\n\n sends  message to the model.\n\n give it:\n\nThe model name (gpt-4o-mini) â€“ a fast and cheap version of GPT-4o.\n\nA list of messages:\n\nsystem: sets the behavior of the AI (e.g., â€œBe helpful and shortâ€)\n\nuser: the prompt you entered\n\ntemperature=0.0: means â€œgive me factual, consistent answersâ€ (no creativity)\n\nresponse = completion.choices[0].message.content\n\nThe AI might return multiple \"choices\", but we take the first one.\n\nWe get just the text of the response (not metadata).\n\nreturn response\n\nSends back the response so use it elsewhere in  program.\n\nTemperature:\n\nTemperature\n\nBehavior\n\nExample\n\n0.0\n\nVery strict and predictable\n\nAlways gives the same answer if you ask the same question\n\n0.5\n\nSomewhat creative\n\nMight say the same thing in different ways\n\n1.0\n\nVery creative and random\n\nTries new ideas, fun writing, maybe less accurate\n\n>1.0\n\nUnstable\n\nRarely used; responses can be unpredictable or strange\n\nEach message has:\n\nrole: who is speaking (system, user, or assistant)\n\ncontent: what they are saying (the actual text)\n\n\n\n\n\n\n\nWorking with LLMs\n\nPython can interact with Large Language Models (LLMs), like GPT, using helper functions.\n\nExample of a helper function:\n\nfrom helper_functions import print_llm_response\n\n\n\nprint_llm_response(\"What is the capital of France?\")\n\nAnother example with a prompt:\n\nfrom helper_functions import *\n\n\n\nname = \"san\"\n\nage = 21.345\n\nprompt = f\"Write a story about {name}. She is intelligent  and childish, age {round(age)}. Tell it in 50 words.\"\n\n\n\nres = get_llm_response(prompt)\n\nprint(res)\n\nPurpose: print_llm_response helps display the LLM output neatly.\n\nsend instructions to the LLM and get a readable response.\n\n\n\nCommon Functions in Python\n\nFunction\n\nUse Case\n\nprint()\n\nDisplay output\n\ninput()\n\nTake input from the user\n\nint()\n\nConvert to integer\n\nfloat()\n\nConvert to float\n\nstr()\n\nConvert to string\n\nbool()\n\nConvert to boolean\n\nabs()\n\nAbsolute value\n\nround()\n\nRound a number\n\npow()\n\nPower calculation\n\nsum()\n\nSum of iterable\n\nlen()\n\nLength of iterable\n\nmax()\n\nMaximum value\n\nmin()\n\nMinimum value\n\nsorted()\n\nSort items\n\ntype()\n\nGet type of object\n\nisinstance()\n\nCheck object type\n\nExamples:\n\nprint(len([1,2,3]))   # 3\n\nprint(type(10))       # <class 'int'>\n\nprint(int(\"5\"))       # 5\n\nprint(sum([1,2,3,4])) # 10\n\n\n\nModule 2:Automating Task with Python:\n\n1.List:\n\nA list is a collection that can store multiple items in a single variable.\n\nMutable\n\nOrdered\n\nSame type or different type\n\nExample:\n\n\n\nlist=[1,2,3]\n\nlist.append(4)\n\nprint(list)\n\nlist.remove(3)\n\nprint(list)\n\nprint(list[1])\n\nprint(list[-1])\n\nOutput: [1, 2, 3, 4]\n\n[1, 2, 4]\n\n2\n\n4\n\n\n\nLLM-based Text and Poetry Generation for Lists:\n\nfrom helper_functions import print_llm_response, get_llm_response\n\nflower_name=[\"rose\",\"lotus\",\"jasmine\"]\n\nprompt=f\"\"\"write a few sentence about the flowers{flower_name}and poem for each flower\"\"\"\n\nres = get_llm_response(prompt)\n\nprint(res)\n\n\n\nfrom helper_functions import print_llm_response, get_llm_response\n\nflower_name=[\"write a poem for tree\",\"write a email for manager about planned leave\",\"give content for any one freedom fighter india\"]\n\nlist=flower_name[1]\n\nprint_llm_response(list)\n\n\n\n2.For loop:\n\nA for loop is used to repeat a block of code for each item in a sequence\n\nflowers = [\"rose\", \"lotus\", \"jasmine\"]\n\nfor flower in flowers:\n\nprint(flower)\n\nLooping through string:\n\nfor char in \"work\":\n\nprint(char)\n\nOutput:\n\nW\n\nO\n\nR\n\nK\n\nRange functions:\n\nThe range() function generates a sequence of numbers.\n\nfor i in range(5):\n\nprint(i)\n\nllm with for loop:\n\n1.from helper_functions import print_llm_response, get_llm_response\n\nlist_of_tasks=[\"write a poem \",\"write a emial about task\"]\n\nfor task in list_of_tasks:\n\nprint_llm_response(task)\n\n\n\n2.from helper_functions import print_llm_response, get_llm_response\n\nlist=[\"football \",\"cricket\",\"volleyball\"]\n\nprompt=f\"\"\"decribe about the sports and marketing for this sports to join Sports:{list}\"\"\"\n\nprint(prompt)\n\nprint_llm_response(prompt)\n\n\n\n3. newlist=[]\n\nfrom helper_functions import print_llm_response, get_llm_response\n\nlist=[\"football \",\"cricket\",\"volleyball\"]\n\nprompt=f\"\"\"decribe about the sports and marketing for this sports to join Sports:{list}\"\"\"\n\ndescription=get_llm_response(prompt)\n\nnewlist.append(description)\n\nprint(newlist)\n\n\n\n3.Dictionaries:\n\nA dictionary in Python is a collection of keyâ€“value pairs.Each key is unique, and it maps to a value\n\n\n\nice_cream_flavors = {\n\n\"Mint Chocolate Chip\": \"Refreshing mint ice cream studded with decadent chocolate chips.\",\n\n\"Cookie Dough\": \"Vanilla ice cream loaded with chunks of chocolate chip cookie dough.\",\n\n\"Salted Caramel\": \"Sweet and salty with a smooth caramel swirl and a hint of sea salt.\"\n\n}\n\n\n\n# Print only keys\n\nprint(\" Ice Cream Flavors (Keys):\")\n\nprint(ice_cream_flavors.keys())   # dict_keys object\n\nprint(list(ice_cream_flavors.values()))  # convert to list for clean output\n\nprint(ice_cream_flavors[\"Mint Chocolate Chip\"])\n\nice_cream_flavors[\"Mint Chocolate Chip\"]=\"mint flavor is good\"\n\nprint(list(ice_cream_flavors.values()))\n\nice_cream_flavors[\"list\"]=[1,2,3]\n\nprint(ice_cream_flavors)\n\n\n\nllm with dic\n\nhigh_priority_tasks = [\n\n\"Compose a brief email to my boss explaining that I will be late for tomorrow's meeting.\",\n\n\"Create an outline for a presentation on the benefits of remote work.\"\n\n]\n\n\n\nmedium_priority_tasks = [\n\n\"Write a birthday poem for Otto, celebrating his 28th birthday.\",\n\n\"Draft a thank-you note for my neighbor Dapinder who helped water my plants while I was on vacation.\"\n\n]\n\n\n\nlow_priority_tasks = [\n\n\"Write a 300-word review of the movie 'The Arrival'.\"\n\n]\n\nprioritized_tasks = {\n\n\"high_priority\": high_priority_tasks,\n\n\"medium_priority\": medium_priority_tasks,\n\n\"low_priority\": low_priority_tasks\n\n}\n\nprint(prioritized_tasks)\n\nfor i in prioritized_tasks[\"high_priority\"]:\n\nprint_llm_response(i)\n\n\n\nBoolean:\n\nBoolean is a data type.contains true and false\n\nMainly used in conditions  and comparrsions.\n\na=True\n\ntype(A)//true\n\nb=False\n\ntype(b)//false\n\nAi make decision based on condition:\n\nfrom helper_functions import print_llm_response\n\ntask=[{\"description\":\"Write a email to manager about task completed details\",\"time\":5},{\"description\":\"Write a email to hr about task planned leave details\",\"time\":10},{\"description\":\"content for ai with python course\",\"time\":5}]\n\nfor s in task:\n\nif s[\"time\"]<5:\n\ndone=s[\"description\"]\n\nprint_llm_response(done)\n\nelse:\n\nprint_llm_response(\"motivate to complete the task daily different different give\")  \n\ndo=s[\"description\"]\n\nprint(f\"\"\"to complete the task{s[\"time\"]}time\"\"\")\n\n2. from helper_functions import print_llm_response\n\ntask=[{\"description\":\"Write a email to manager about task completed details\",\"time\":5},{\"description\":\"Write a email to hr about task planned leave details\",\"time\":10},{\"description\":\"content for ai with python course\",\"time\":5}]\n\nneed_to_do=[]\n\nfor s in task:\n\nif s[\"time\"]<5:\n\ndone=s[\"description\"]\n\nprint_llm_response(done)\n\nelse:\n\nprint_llm_response(\"motivate to complete the task daily different different give\")  \n\ndo=s[\"description\"]\n\nprint(f\"\"\"to complete the task{s[\"time\"]}time\"\"\")\n\nneed_to_do.append(s)\n\nprint(s)\n\nModule3:Working with data and spreadsheet in Ai\n\nIPython.display:\n\nIPython.display is a module that helps you display rich content (HTML, images, audio, video, Markdown, widgets, etc.) inside Jupyter Notebook or IPython shell. display() â†’ can show formatted text, pictures, audio, video, tables, HTML.\n\nUse Ipython for mardowna nad format:\n\nfrom helper_functions import get_llm_response\n\nfrom IPython.display import display, Markdown\n\nlist=[\"briyani\",\"Idly\",\"Icecream\"]\n\nprompt=f\"\"\"describe the recipe and how to make those integredient{list}\"\"\"\n\nget_prompt1=get_llm_response(prompt)\n\nprint(get_prompt1)\n\n\n\n\n\nOpen email in text format read mode:\n\n\n\nf = open(\"email.txt\", \"r\")\n\nemail = f.read()\n\nf.close()\n\n\n\n\"r\" â†’ read mode\n\nread() â†’ read all content as a string\n\nclose() â†’ close the file to free resources \n\nin this we manually need to close the file.\n\n\n\nAutomatically close the email by using with context block\n\n\n\n\n\nwith open(\"email.txt\", \"r\") as f:  # <-- start of context block\n\nemail = f.read()\n\nprint(email)\n\n# <-- end of context block, file is automatically closed\n\n\n\nInside the block:  read the file safely.\n\nAfter the block: Python automatically closes the file (f.close() happens ).\n\n\n\n\n\nFunction\n\nPurpose\n\nupload_txt_file\n\nHandles uploading or saving a text file to a folder or server. This prevents  from writing the file upload logic multiple times.\n\nlist_files_in_directory\n\nReturns a list of all files in a folder. Useful if  want to check available files, iterate over them, or display them in your app.\n\nprint_llm_response\n\nupload_txt_file()ðŸ¡ªupload a fie\n\nlist_files_in_directory()__>list all files\n\n\n\nPrints the response from a large language model (LLM) in a clean, readable format. Helps separate display logic from main code.\n\n\n\n\n\n\n\nAbove three not predefined functions in python:my_project/\n\nâ”‚\n\nâ”œâ”€â”€ main.py\n\nâ”œâ”€â”€ helper_functions.py\n\nâ””â”€â”€ uploads/   <-- folder we will create/use\n\nimport os\n\n\n\ndef upload_txt_file(file_path, destination):\n\n\"\"\"Copy a text file to the destination folder\"\"\"\n\nwith open(file_path, \"r\") as f:\n\ncontent = f.read()\n\nos.makedirs(destination, exist_ok=True)  # create folder if not exists\n\nwith open(os.path.join(destination, os.path.basename(file_path)), \"w\") as f2:\n\nf2.write(content)\n\nprint(f\"{file_path} uploaded to {destination}\")\n\n\n\ndef list_files_in_directory(folder):\n\n\"\"\"Return a list of all files in the folder\"\"\"\n\nreturn os.listdir(folder)\n\n\n\ndef print_llm_response(response):\n\n\"\"\"Print LLM response nicely\"\"\"\n\nprint(\"\\nLLM Response:\\n\" + response + \"\\n\")\n\n\n\nmain.py(locally run this file)\n\nfrom helper_functions import upload_txt_file, list_files_in_directory, print_llm_response\n\nimport os\n\n\n\n# 1ï¸âƒ£ Create folder 'uploads' (if not exists)\n\nos.makedirs(\"uploads\", exist_ok=True)\n\n\n\n# 2ï¸âƒ£ Make sure a file exists to upload (create one manually for test)\n\n# Example: create 'email.txt' in your project folder with some text\n\n\n\n# 3ï¸âƒ£ Upload file to 'uploads'\n\nupload_txt_file(\"email.txt\", \"uploads\")  # email.txt must exist in your project folder\n\n\n\n# 4ï¸âƒ£ List all files in 'uploads'\n\nfiles = list_files_in_directory(\"uploads\")\n\nprint(\"Files in folder:\", files)\n\n\n\n# 5ï¸âƒ£ Print an LLM-style response\n\nllm_response = f\"{files[0]} successfully uploaded and listed in the folder.\"\n\nprint_llm_response(llm_response)\n\n\n\n\n\nfrom helper_functions import get_llm_response, print_llm_response\n\nf = open(\"cape_town.txt\", \"r\")\n\njournal_cape_town = f.read()\n\nf.close()\n\nprint(journal_cape_town)\n\n\n\n\n\nRead a multiple file and give which restaurants suitable for food if suitable for food give the food detailes\n\nfrom helper_functions import get_llm_response, print_llm_response\n\n\n\nfiles = [\"cape_town.txt\", \"madrid.txt\", \"rio_de_janeiro.txt\", \"sydney.txt\", \"tokyo.txt\"]\n\nfor file in files:\n\n# Read journal file for the city\n\nf = open(file, \"r\")\n\njournal = f.read()\n\nf.close()\n\n\n\n# Create prompt\n\nprompt = f\"\"\"Respond with \"Relevant to food\" or \"Not relevant to food\":if revelant mention what dish is special else visited another restaturant like this.  \n\nthe journal describes restaurants and their specialties \n\nJournal:\n\n{journal}\"\"\"\n\n\n\n# Use LLM to determine if the journal entry is useful\n\nprint(f\"{file} -> {get_llm_response(prompt)}\")\n\n\n\n\n\n\n\n\n\nfrom helper_functions import *\n\nfrom IPython.display import display, HTML\n\nfiles = [\"cape_town.txt\", \"istanbul.txt\", \"new_york.txt\", \"paris.txt\", \n\n\"rio_de_janeiro.txt\", \"sydney.txt\", \"tokyo.txt\"]\n\n\n\nfor file in files:\n\n#Open file and read contents\n\njournal_entry = read_journal(file)\n\n\n\n#Extract restaurants and display csv\n\nprompt = f\"\"\"\n\nExtract all restaurants, dishes, and chefs from the following journal entry.\n\nProvide the output in strict CSV format with columns:\n\n\n\nRestaurant,Dish,Chef\n\n\n\nRules:\n\nInclude headers exactly as above.\n\nIf the chef is not mentioned, write N/A.\n\nDo not include any markdown or extra explanations.\n\nKeep dish names as plain text (we will color them later in HTML).\n\nOutput only CSV rows.\n\n\n\nJournal entry:\n\n{journal_entry}\n\n\"\"\"\n\n\n\n\n\nprint(file)\n\nprint_llm_response(prompt)\n\nprint(\"\") # Prints a blank line!\n\n\n\nPrompt for hightlight the sentence:\n\nprompt = f\"\"\"\n\nGiven the following journal entry from a food critic, identify the \n\nrestaurants and their best dishes. Highlight and bold each restaurant \n\n(in orange) and best dish (in blue) within the original text. \n\n\n\nProvide the output as HTML suitable for display in a Jupyter notebook. \n\n\n\nJournal entry:\n\n{journal_tokyo}\n\n\"\"\"\n\nFor download the particular file\n\ndownload_file()\n\n\n\nCSV file:\n\nfrom helper_functions import get_llm_response, print_llm_response, display_table\n\nfrom IPython.display import Markdown\n\nimport csv\n\nf = open(\"itinerary.csv\", 'r')\n\ncsv_reader = csv.DictReader(f)\n\nitinerary = []\n\nfor row in csv_reader:\n\nprint(row)\n\nitinerary.append(row)\n\nf.close()\n\nprint(itinerary)\n\nprint(itinerary[0])\n\nprint(itinerary[0][\"Country\"])\n\ndisplay_table(itinerary)\n\n\n\nThe code opens the file itinerary.csv in read mode.\n\nIt uses csv.DictReader to read each row as a dictionary (key = column name, value = cell data).\n\nAll rows are collected into a list called itinerary.\n\nIt prints each row, the full list, and the first row.\n\nIt also shows how to access a specific value (Country from the first row).\n\nFinally, display_table(itinerary) displays the data neatly in table format.\n\n\n\nDisplay data based on condition\n\nfrom helper_functions import get_llm_response, print_llm_response, display_table\n\nfrom IPython.display import Markdown\n\nimport csv\n\nf = open(\"itinerary.csv\", 'r')\n\ncsv_reader = csv.DictReader(f)\n\nfiltered_data = []\n\n\n\n# Filter by country\n\nfor trip_stop in itinerary:\n\n# For example: get the destinations located in \"Japan\"\n\nif trip_stop[\"Country\"] == \"JAPAN\":\n\nfiltered_data.append(trip_stop)\n\ndisplay_table(filtered_data)\n\n\n\nOpens the CSV file (itinerary.csv).\n\nReads each row as a dictionary (column â†’ value).\n\nGoes through all rows one by one.\n\nSelects only the rows where \"Country\" is \"JAPAN\".\n\nSaves those rows into filtered_data.\n\nDisplays the filtered results as a table.\n\nwith open(\"itinerary.csv\", 'r') as f:\n\ncsv_reader = csv.DictReader(f)\n\nitinerary = list(csv_reader)  # Convert to a list\n\n\n\nif itinerary:  # Check if list is not empty\n\ntrip_stop = itinerary[1]  # Access the first item\n\ncity = trip_stop[\"City\"]\n\ncountry = trip_stop[\"Country\"]\n\narrival = trip_stop[\"Arrival\"]\n\ndeparture = trip_stop[\"Departure\"]\n\nprompt = f\"\"\"I will visit {city}, {country}, from {arrival} to {departure}. \n\nPlease create a detailed daily itinerary.\"\"\"\n\nprint(prompt)\n\nelse:\n\nprint(\"No data found in the CSV file.\"),\n\n\n\n\n\n\n\nwith open(\"itinerary.csv\", 'r') as f:       \n\ncsv_reader = csv.DictReader(f)          \n\nitinerary = list(csv_reader)            \n\n\n\nif itinerary:                               \n\ntrip_stop = itinerary[1]                \n\ncity = trip_stop[\"City\"]                \n\ncountry = trip_stop[\"Country\"]          \n\narrival = trip_stop[\"Arrival\"]          \n\ndeparture = trip_stop[\"Departure\"]      \n\nprompt = f\"\"\"I will visit {city}, {country}, from {arrival} to {departure}. \n\nPlease create a detailed daily itinerary.\"\"\"  \n\nprint(prompt)                           \n\nelse:\n\nprint(\"No data found in the CSV file.\") \n\nwith open(\"itinerary.csv\", 'r') as f:Opens the file itinerary.csv in read mode safely (auto closes when done).\n\ncsv.DictReader(f)Reads the CSV file row by row, turning each row into a dictionary.\n\nlist(csv_reader)Converts all rows into a list of dictionaries called itinerary.\n\nif itinerary:Checks if the CSV had any data (not empty).\n\ntrip_stop = itinerary[1]Takes the second row from the CSV (because indexing starts at 0).\n\n6â€“9. Extracts values from that row:\n\n\"City\" â†’ stored in city\n\n\"Country\" â†’ stored in country\n\n\"Arrival\" â†’ stored in arrival\n\n\"Departure\" â†’ stored in departure\n\nCreates a prompt sentence using those values.\n\nPrints the generated prompt.\n\nIf the CSV file is empty â†’ prints \"No data found in the CSV file.\"\n\n\n\n\n\n\n\nDefine functions for read and display the data in table format:\n\nimport csv\n\nfrom helper_functions import print_llm_response, get_llm_response, display_table\n\nfrom IPython.display import Markdown\n\n# The function called 'read_journal'\n\ndef read_journals(journal_file):\n\nf = open(journal_file, \"r\")\n\njournal = f.read() \n\nf.close()\n\n\n\n# Return the journal content\n\nreturn journal\n\n\n\njournal = read_journals(\"sydney.txt\")\n\n\n\nprint(journal)\n\nsydney_restaurants = read_csv(\"Sydney.csv\")\n\nfrom IPython.display import display, HTML\n\n\n\ndef display_table(data):\n\n\"\"\"\n\nDisplay a list of dictionaries as a HTML table.\n\nEach dictionary is a row, keys are column headers.\n\n\"\"\"\n\nif not data:\n\nprint(\"No data to display.\")\n\nreturn\n\n\n\n# Create table headers from the first dictionary\n\nheaders = data[0].keys()\n\n\n\n# Start HTML table\n\nhtml = \"<table border='1' style='border-collapse: collapse;'>\"\n\n\n\n# Add header row\n\nhtml += \"<tr>\"\n\nfor header in headers:\n\nhtml += f\"<th style='padding:5px; background-color:#f2f2f2;'>{header}</th>\"\n\nhtml += \"</tr>\"\n\n\n\n# Add data rows\n\nfor row in data:\n\nhtml += \"<tr>\"\n\nfor header in headers:\n\nhtml += f\"<td style='padding:5px;'>{row[header]}</td>\"\n\nhtml += \"</tr>\"\n\n\n\nhtml += \"</table>\"\n\n\n\n# Display the table\n\ndisplay(HTML(html))\n\nprint(\"----------------------\")\n\ndisplay_table(sydney_restaurants)\\\n\n\n\nOutput:\n\n\n\n\n\nModules 4:extending python with packages and apiâ€™s\n\n\n\n\n\nHelperfunction.py\n\n\n\n\n\nimport os\n\nfrom openai import OpenAI\n\nfrom dotenv import load_dotenv\n\n\n\nclient = NoneÂ  # declare globally\n\n\n\n\n\ndef authenticate(api_key=None):\n\nglobal client\n\ntry:\n\n# Load environment variables from .env file\n\nload_dotenv('.env', override=True)\n\n\n\n# Get API key from environment\n\nopenai_api_key = os.getenv('OPENAI_API_KEY')\n\n\n\n# Use key from .env if available, else fallback to passed argument\n\nif openai_api_key:\n\nclient = OpenAI(api_key=openai_api_key)\n\nelif api_key:\n\nclient = OpenAI(api_key=api_key)\n\nelse:\n\nprint(\n\n\"Â Warning: An OpenAI API key is required to use AI model functions.\\n\"\n\n\"Please provide the key by calling `authenticate(openai_api_key)` \"\n\n\"or ensure it is specified in the `.env` file as 'OPENAI_API_KEY'.\\n\"\n\n\"If you set it in the `.env` file, call `authenticate()` or reload the package to proceed.\"\n\n)\n\nreturn\n\n\n\nprint(\"Authentication successful!\")\n\n\n\nexcept Exception as e:\n\nprint(f\"Â Authentication failed: {e}\")\n\n\n\n\n\n# Call function (no argument means it will try to read from .env)\n\nauthenticate()\n\ndef print_llm_response(prompt):\n\n\"\"\"\n\nThis function takes a prompt (a string),\n\nsends it to OpenAI's GPT-4o-mini model,\n\nand prints the model's response.\n\n\"\"\"\n\ntry:\n\n# Validate input type\n\nif not isinstance(prompt, str):\n\nraise ValueError(\"Input must be a string enclosed in quotes.\")\n\n\n\n# Send request to OpenAI model\n\ncompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": \"You are a helpful but terse AI assistant who gets straight to the point.\"},\n\n{\"role\": \"user\", \"content\": prompt}\n\n],\n\ntemperature=0.0 # deterministic output\n\n)\n\n\n\n# Extract response from model\n\nresponse = completion.choices[0].message.content\n\nprint(response)\n\n\n\nexcept Exception as e:\n\nprint(\"Error:\", str(e))\n\ndef get_llm_response(prompt):\n\n\"\"\"\n\nThis function takes a prompt (a string),\n\nsends it to OpenAI's GPT-4o-mini model,\n\nand returns the response as a string.\n\n\"\"\"\n\ntry:\n\nif not isinstance(prompt, str):\n\nraise ValueError(\"Input must be a string enclosed in quotes.\")\n\n\n\ncompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\"role\": \"system\", \"content\": \"You are a helpful but terse AI assistant who gets straight to the point.\"},\n\n{\"role\": \"user\", \"content\": prompt}\n\n],\n\ntemperature=0.0\n\n)\n\n\n\nresponse = completion.choices[0].message.content\n\nreturn response\n\n\n\nexcept Exception as e:\n\nreturn f\"Error: {str(e)}\"\n\n\n\n\n\ndef cels_to_fahrenheit(celsius):\n\n\"\"\"\n\nConvert Celsius temperature to Fahrenheit\n\nand print the result.\n\n\"\"\"\n\nfahrenheit = (celsius * 9/5) + 32\n\nprint(f\"{celsius}Â°C is equivalent to {fahrenheit:.2f}Â°F\")\n\n\n\n\n\nfrom helper_functions import *\n\nprint_llm_response(\"who is the prime minister of india today\")\n\nprint(celsius_to_fahrenheit(30))\n\nget=get_llm_response(\"father of science\")\n\nprint(get)\n\nOutput:\n\n\n\n\n\n\n\nMath Package\n\nFunctions like cos, sin, tan, pi, floor trigonometry and rounding.\n\nExample 1: Cosine\n\nfrom math import cos, pi\n\n\n\n# Calculate cosine of pi/2\n\nprint(cos(pi/2))  # Output: 0.0\n\nExample 2: Floor\n\nfrom math import floor\n\n\n\n# Round down a number\n\nprint(floor(5.7))  # Output: 5\n\nExample 3: Tangent\n\nfrom math import tan\n\n\n\nvalues = [0, pi/4, pi/2]\n\nfor value in values:\n\nprint(f\"Tangent of {value:.2f} is {tan(value)}\")\n\n\n\nStatistics Package\n\nFunctions like mean, median, stdev help calculate average and spread of data.\n\nExample 1: Mean\n\nfrom statistics import mean\n\n\n\nheights = [160, 172, 155]\n\nprint(mean(heights))  # Output: 162.33\n\nExample 2: Standard Deviation\n\nfrom statistics import stdev\n\n\n\nprint(stdev(heights))  # Output: 8.5\n\nExample 3: Median\n\nfrom statistics import median\n\n\n\nscores = [28, 14, 15, 25, 21]\n\nprint(median(scores))  # Output: 21\n\n\n\nRandom Package\n\nFunctions like sample or randint generate random numbers or select random items from a list.\n\nExample 1: Random Sample\n\nfrom random import sample\n\n\n\nspices = [\"cumin\", \"turmeric\", \"oregano\", \"paprika\"]\n\nprint(sample(spices, 2))  # Output: ['oregano', 'cumin'] (random)\n\nExample 2: Random Integer\n\nfrom random import randint\n\n\n\nprint(randint(1, 10))  # Output: random number between 1 and 10\n\n\n\nUsing LLM with Random Ingredients\n\ncombine random selections with an LLM to generate recipes.\n\nExample:\n\nfrom random import sample\n\nfrom helper_functions import get_llm_response\n\n\n\nspices = [\"cumin\", \"turmeric\", \"oregano\", \"paprika\"]\n\nvegetables = [\"lettuce\", \"tomato\", \"carrot\", \"broccoli\"]\n\nproteins = [\"chicken\", \"tofu\", \"beef\", \"fish\"]\n\n\n\n# Randomly select ingredients\n\nrandom_spices = sample(spices, 2)\n\nrandom_vegetables = sample(vegetables, 2)\n\nrandom_protein = sample(proteins, 1)\n\n\n\n# Create prompt\n\nprompt = f\"\"\"Please suggest a recipe using:\n\nSpices: {random_spices}\n\nVegetables: {random_vegetables}\n\nProteins: {random_protein}\"\"\"\n\n\n\n# Generate recipe from LLM\n\nrecipe = get_llm_response(prompt)\n\nprint(recipe)\n\n\n\nThird party package\n\n\n\nPandas = Python + Excel-like tables\n\nDataFrame = table, Series = column\n\nUse Pandas for reading, analyzing, filtering, and saving structured data\n\nMain Use of Pandas\n\nPandas is mainly used for working with structured data (like tables).\n\nYou can use it to:\n\nRead and write data â€“ from CSV, Excel, JSON, SQL, etc.\n\nAnalyze data â€“ calculate averages, sums, max/min, counts, etc.\n\nClean data â€“ remove duplicates, fill missing values, filter rows.\n\nManipulate data â€“ sort, group, merge, or transform columns.\n\nPrepare data â€“ for machine learning, visualization, or reports.\n\nHow to Use Pandas (Basic Steps)\n\nInstall and Import\n\npip install pandas\n\nimport pandas as pd\n\nRead a CSV File\n\ndf = pd.read_csv(\"data.csv\")  # Load CSV into a DataFrame\n\nView Data\n\nprint(df.head())      # Show first 5 rows\n\nprint(df.columns)     # Show column names\n\nAccess Columns and Rows\n\nprint(df['Name'])     # Access column 'Name'\n\nprint(df.iloc[0])     # Access first row\n\nFilter Data\n\nadults = df[df['Age'] > 18]   # Only rows where Age > 18\n\nBasic Analysis\n\nprint(df['Age'].mean())       # Average age\n\nprint(df['Salary'].max())     # Maximum salary\n\nSave to CSV\n\ndf.to_csv(\"filtered_data.csv\", index=False)\n\nexample:\n\n\n\nimport pandas as pd\n\ndata = pd.read_csv('car_data.csv')\n\nprint(data.head())      # Show first 5 rows\n\nprint(data.columns) \n\nprint(data['Price'])\n\nprint(data.iloc[0])\n\nprint(data[data['Price'] > 12550])   \n\nprint(data['Price'].mean())      \n\nprint(data['Price'].max())\n\n\n\nWhat is Matplotlib?\n\nMatplotlib is a third-party Python library for data visualization.\n\nIt helps you create graphs, charts, and plots from your data.\n\nVery useful for exploring data and presenting results.\n\n\n\n2ï¸âƒ£ Main Uses\n\nLine plots â€“ to show trends over time.\n\nBar charts â€“ to compare quantities.\n\nScatter plots â€“ to show relationships between variables.\n\nHistograms â€“ to show distribution of data.\n\nCustom plots â€“ colors, labels, titles, and legends\n\n# Note the .pyplot\n\nimport matplotlib.pyplot as plt\n\nplt.plot(data[\"Kilometer\"], data[\"Price\"]) \n\nplt.title(\"Line Plot Example\")   # Add title\n\nplt.xlabel(\"X-axis\")             # X-axis label\n\nplt.ylabel(\"Y-axis\")  \n\nplt.grid()# Y-axis label\n\nplt.show()   \n\nprint(\"----------\")\n\n# Display the bar# Create line plot\n\nplt.bar(data[\"Kilometer\"], data[\"Price\"]) \n\nplt.title(\"Line Plot Example\")   # Add title\n\nplt.xlabel(\"X-axis\")             # X-axis label\n\nplt.ylabel(\"Y-axis\")  \n\nplt.grid()\n\n# Y-axis label\n\nplt.show()     \n\n# Display the plot# Create line plot\n\nplt.scatter(data[\"Kilometer\"], data[\"Price\"]) \n\nplt.title(\"Line Plot Example\")   # Add title\n\nplt.xlabel(\"X-axis\")             # X-axis label\n\nplt.ylabel(\"Y-axis\") \n\nplt.grid(True)\n\nplt.show()   \n\nprint(\"----------\")\n\nplt.hist(data[\"Price\"], bins=5, color='green')\n\nplt.title(\"Price Distribution\")\n\nplt.grid(True)\n\nplt.show()\n\n\n\nBeautifulSoup (bs4)\n\nPurpose: bs4 (BeautifulSoup version 4) is a Python library for web scraping.\n\nUse: It helps you extract data from HTML or XML files, like text, links, tables, or images.\n\nWorks With: Usually used with requests to fetch web pages from the internet.\n\n# The url from one of the Batch's newsletter\n\nurl = 'https://www.deeplearning.ai/the-batch/the-world-needs-more-intelligence/'\n\n\n\n# Getting the content from the webpage's contents\n\nresponse = requests.get(url)\n\n\n\n# Print the response from the requests\n\nprint(response)\n\nHTML(f'<iframe src={url} width=\"60%\" height=\"400\"></iframe>')\n\n# Using beautifulsoup to extract the text\n\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Find all the text in paragraph elements on the webpage\n\nall_text = soup.find_all('h1')\n\n\n\n# Create an empty string to store the extracted text\n\ncombined_text = \"\"\n\n\n\n# Iterate over 'all_text' and add to the combined_text string\n\nfor text in all_text:\n\ncombined_text = combined_text + \"\\n\" + text.get_text()\n\n\n\n# Print the final combined text\n\nprint(combined_text)\n\nprompt = f\"\"\"Extract the key bullet points from the following text.\n\n\n\nText:\n\n{combined_text}\n\n\"\"\"\n\nprint_llm_response(prompt)\n\n\n\nOutput:\n\n<Response [200]>\n\n\n\nThe World Needs More Intelligence Human intelligence is expensive, artificial intelligence is cheap. To solve big problems like climate change, it makes sense to double down on AI.\n\nHuman intelligence is expensive.\n\nArtificial intelligence is cheap.\n\nAI can help solve big problems like climate change.\n\nEmphasis on increasing investment in AI.\n\nAisetup packages:\n\nThe aisetup package is designed to make it easier to interact with LLMs (like OpenAI GPT models).\n\nFunction\n\nPurpose\n\nExample\n\nget_llm_response(prompt)\n\nSends a text prompt to the LLM and returns the generated response.\n\npython\\nfrom aisetup import get_llm_response\\nresponse = get_llm_response(\"Why is Python easy?\")\\nprint(response)\n\nprint_llm_response(prompt)\n\nSends a text prompt to the LLM and prints the response directly.\n\npython\\nfrom aisetup import print_llm_response\\nprint_llm_response(\"Who is the sports minister of Tamil Nadu?\")\n\n!pip install aisetup\n\nfrom aisetup import get_llm_response,print_llm_response,display_table\n\nr=get_llm_response(\"why python easy\")\n\nprint(r)\n\nprint_llm_response(\"who is the sports minister of tamil nadu\")\n\noutput: Requirement already satisfied: aisetup in /usr/local/lib/python3.9/site-packages (0.1.4)\n\nRequirement already satisfied: folium<0.18.0,>=0.17.0 in /usr/local/lib/python3.9/site-packages (from aisetup) (0.17.0)\n\nRequirement already satisfied: ipython==8.18.1 in /usr/local/lib/python3.9/site-packages (from aisetup) (8.18.1)\n\nRequirement already satisfied: ipywidgets<9.0.0,>=8.1.3 in /usr/local/lib/python3.9/site-packages (from aisetup) (8.1.5)\n\nRequirement already satisfied: matplotlib<4.0.0,>=3.9.2 in /usr/local/lib/python3.9/site-packages (from aisetup) (3.9.2)\n\nRequirement already satisfied: numpy==2.0.1 in /usr/local/lib/python3.9/site-packages (from aisetup) (2.0.1)\n\nRequirement already satisfied: openai<2.0.0,>=1.42.0 in /usr/local/lib/python3.9/site-packages (from aisetup) (1.93.0)\n\nRequirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.9/site-packages (from aisetup) (1.0.1)\n\nRequirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.9/site-packages (from aisetup) (2.32.4)\n\nRequirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (5.1.1)\n\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (0.19.1)\n\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (0.1.7)\n\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (3.0.43)\n\nRequirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (2.17.2)\n\nRequirement already satisfied: stack-data in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (0.6.3)\n\nRequirement already satisfied: traitlets>=5 in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (5.9.0)\n\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (4.11.0)\n\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (1.2.2)\n\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/site-packages (from ipython==8.18.1->aisetup) (4.9.0)\n\nRequirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.9/site-packages (from folium<0.18.0,>=0.17.0->aisetup) (0.8.1)\n\nRequirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.9/site-packages (from folium<0.18.0,>=0.17.0->aisetup) (3.1.4)\n\nRequirement already satisfied: xyzservices in /usr/local/lib/python3.9/site-packages (from folium<0.18.0,>=0.17.0->aisetup) (2025.4.0)\n\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.9/site-packages (from ipywidgets<9.0.0,>=8.1.3->aisetup) (0.2.2)\n\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.9/site-packages (from ipywidgets<9.0.0,>=8.1.3->aisetup) (4.0.13)\n\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.9/site-packages (from ipywidgets<9.0.0,>=8.1.3->aisetup) (3.0.13)\n\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (1.3.0)\n\n [notice] A new release of pip is available: 24.0 -> 25.2\n\n[notice] To update, run: pip install --upgrade pip\n\nPython is considered easy due to its simple syntax, readability, and extensive libraries. It allows for quick learning and rapid development, making it accessible for beginners and efficient for experienced programmers.\n\npython-dotenv\n\nThe package is installed as:\n\npip install python-dotenv\n\nIt allows store sensitive data like API keys, passwords, or configuration settings in a .env file instead of hardcoding them in your scripts.\n\n\n\nThis keeps your code secure and portable.\n\n# Install required packages (run only once)\n\n!pip install geocoder requests\n\n\n\nimport geocoder\n\nimport requests\n\n\n\ndef get_feels_like_temperature(api_key):\n\nif not api_key:\n\nprint(\" API key not provided.\")\n\nreturn\n\n\n\n# Get current location\n\ng = geocoder.ip('me')\n\nif not g.ok:\n\nprint(\" Could not detect location.\")\n\nreturn\n\n\n\nlat, lon = g.latlng\n\nprint(f\"Detected Latitude: {lat}, Longitude: {lon}\")\n\n\n\n# Use the \"weather\" endpoint for current temperature\n\nurl = f\"https://api.openweathermap.org/data/2.5/weather?units=metric&lat={lat}&lon={lon}&appid={api_key}\"\n\n\n\n# Make API request\n\nresponse = requests.get(url)\n\nif response.status_code != 200:\n\nprint(f\" Error fetching weather data: {response.status_code}\")\n\nprint(response.text)  # Show API error message\n\nreturn\n\n\n\ndata = response.json()\n\nfeels_like = data['main']['feels_like']\n\ncity = data['name']\n\n\n\nprint(f\" The temperature currently feels like {feels_like}Â°C in {city}.\")\n\n\n\n# Call the function with your API key\n\nget_feels_like_temperature(\"xyxsddfff\")\n\nOpenAi:\n\nThe openai package is the official Python client library provided by OpenAI.It allows you to connect your Python programs to OpenAIâ€™s models like GPT (ChatGPT), DALLÂ·E, Whisper, and Embeddings through API calls.\n\nimport os\n\nfrom dotenv import load_dotenv\n\nfrom openai import OpenAI\n\n\n\n# Load environment variables from .env file\n\nload_dotenv('.env', override=True)\n\n\n\n# Get API key from environment\n\nopenai_api_key = os.getenv('OPENAI_API_KEY')\n\n\n\n# Initialize OpenAI client\n\nclient = OpenAI(api_key=openai_api_key)\n\n\n\n# Function to get LLM response\n\ndef get_llm_response(prompt: str) -> str:\n\ncompletion = client.chat.completions.create(\n\nmodel=\"gpt-4o-mini\",\n\nmessages=[\n\n{\n\n\"role\": \"system\",\n\n\"content\": \"You are a helpful but terse AI assistant who gets straight to the point.\",\n\n},\n\n{\"role\": \"user\", \"content\": prompt},\n\n],\n\ntemperature=0.0,\n\n)\n\nresponse = completion.choices[0].message.content\n\nreturn response\n\n\n\n# Example usage\n\nprint(get_llm_response(\"Who are you? Who is the president of India?\"))\n\nwhen temperature change I content also changed\n\nResources:LLm create \n\nBeginnerâ€™s Guide to OpenAI API. Build your own LLM tool from scratch | by Chanin Nantasenamat | Data Professor | Medium\n\n\n\n\n\n\n\n",
        "detectedPatterns": [
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value"
        ],
        "keyValuePairs": {
          "Course Name": "AI Python for Beginners",
          "print(f\"The addition of a + b is {a+b}\")  # Output": "The addition of a + b is 30",
          "print(f\"Pi": "{pi:.2f}\")   # Pi: 3.14",
          "print(f\"Number": "{num:,}\") # Number: 12,345",
          "print(f\"{age=}\")  # Output": "age=30",
          "Examples": "",
          "override=True means": "If there's already an environment variable, replace it with the one in .env.",
          "def get_llm_response(prompt": "str) -> str:",
          "system": "sets the behavior of the AI (e.g., â€œBe helpful and shortâ€)",
          "user": "the prompt you entered",
          "temperature=0.0": "means â€œgive me factual, consistent answersâ€ (no creativity)",
          "role": "who is speaking (system, user, or assistant)",
          "content": "what they are saying (the actual text)",
          "Purpose": "bs4 (BeautifulSoup version 4) is a Python library for web scraping.",
          "Module 2": "Automating Task with Python:",
          "Output": "[1, 2, 3, 4]",
          "prompt=f\"\"\"decribe about the sports and marketing for this sports to join Sports": "{list}\"\"\"",
          "\"Mint Chocolate Chip\"": "\"Refreshing mint ice cream studded with decadent chocolate chips.\",",
          "\"Cookie Dough\"": "\"Vanilla ice cream loaded with chunks of chocolate chip cookie dough.\",",
          "\"Salted Caramel\"": "\"Sweet and salty with a smooth caramel swirl and a hint of sea salt.\"",
          "print(\" Ice Cream Flavors (Keys)": "\")",
          "\"high_priority\"": "high_priority_tasks,",
          "\"medium_priority\"": "medium_priority_tasks,",
          "\"low_priority\"": "low_priority_tasks",
          "task=[{\"description\"": "\"Write a email to manager about task completed details\",\"time\":5},{\"description\":\"Write a email to hr about task planned leave details\",\"time\":10},{\"description\":\"content for ai with python course\",\"time\":5}]",
          "Module3": "Working with data and spreadsheet in Ai",
          "with open(\"email.txt\", \"r\") as f": "# <-- start of context block",
          "Inside the block": "read the file safely.",
          "After the block": "Python automatically closes the file (f.close() happens ).",
          "Above three not predefined functions in python": "my_project/",
          "print(\"\\nLLM Response": "\\n\" + response + \"\\n\")",
          "# Example": "create 'email.txt' in your project folder with some text",
          "print(\"Files in folder": "\", files)",
          "prompt = f\"\"\"Respond with \"Relevant to food\" or \"Not relevant to food\"": "if revelant mention what dish is special else visited another restaturant like this.",
          "# For example": "get the destinations located in \"Japan\"",
          "if itinerary": "Checks if the CSV had any data (not empty).",
          "with open(\"itinerary.csv\", 'r') as f": "Opens the file itinerary.csv in read mode safely (auto closes when done).",
          "html = \"<table border='1' style='border-collapse": "collapse;'>\"",
          "html += f\"<th style='padding": "5px; background-color:#f2f2f2;'>{header}</th>\"",
          "html += f\"<td style='padding": "5px;'>{row[header]}</td>\"",
          "Modules 4": "extending python with packages and apiâ€™s",
          "\"Â Warning": "An OpenAI API key is required to use AI model functions.\\n\"",
          "print(f\"Â Authentication failed": "{e}\")",
          "{\"role\"": "\"user\", \"content\": prompt},",
          "print(\"Error": "\", str(e))",
          "return f\"Error": "{str(e)}\"",
          "print(f\"{celsius}Â°C is equivalent to {fahrenheit": ".2f}Â°F\")",
          "Example 1": "Random Sample",
          "print(cos(pi/2))  # Output": "0.0",
          "Example 2": "Random Integer",
          "print(floor(5.7))  # Output": "5",
          "Example 3": "Median",
          "print(f\"Tangent of {value": ".2f} is {tan(value)}\")",
          "print(mean(heights))  # Output": "162.33",
          "print(stdev(heights))  # Output": "8.5",
          "print(median(scores))  # Output": "21",
          "print(sample(spices, 2))  # Output": "['oregano', 'cumin'] (random)",
          "print(randint(1, 10))  # Output": "random number between 1 and 10",
          "Spices": "{random_spices}",
          "Vegetables": "{random_vegetables}",
          "Proteins": "{random_protein}\"\"\"",
          "Use": "It helps you extract data from HTML or XML files, like text, links, tables, or images.",
          "Works With": "Usually used with requests to fetch web pages from the internet.",
          "url = 'https": "//www.deeplearning.ai/the-batch/the-world-needs-more-intelligence/'",
          "output": "Requirement already satisfied: aisetup in /usr/local/lib/python3.9/site-packages (0.1.4)",
          "Requirement already satisfied": "contourpy>=1.0.1 in /usr/local/lib/python3.9/site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (1.3.0)",
          "[notice] A new release of pip is available": "24.0 -> 25.2",
          "[notice] To update, run": "pip install --upgrade pip",
          "print(f\"Detected Latitude": "{lat}, Longitude: {lon}\")",
          "url = f\"https": "//api.openweathermap.org/data/2.5/weather?units=metric&lat={lat}&lon={lon}&appid={api_key}\"",
          "print(f\" Error fetching weather data": "{response.status_code}\")",
          "\"role\"": "\"system\",",
          "\"content\"": "\"You are a helpful but terse AI assistant who gets straight to the point.\",",
          "Resources": "LLm create"
        }
      }
    },
    {
      "filename": "Generative AI Concepts (1).docx",
      "fileType": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
      "extension": "docx",
      "size": 25097,
      "lastModified": "2025-11-27T05:14:27.173Z",
      "extractedAt": "2025-11-27T10:45:08.245Z",
      "data": {
        "type": "unstructured_text",
        "lineCount": 237,
        "content": "Generative AI Concepts\n\n\n\n1. Natural Language Processing (NLP)\n\nOverview\n\nNatural Language Processing (NLP) is a field of AI that enables machines to understand, interpret, and generate human language.Itâ€™s the foundation for most Generative AI applications â€” from chatbots to document summarization.\n\n\n\nA. Text Processing\n\nBefore any model can process text, raw data must be cleaned and standardized.\n\nSteps in Text Preprocessing\n\nLowercasingConverts all characters to lowercase to avoid treating words like â€œChatGPTâ€ and â€œchatgptâ€ differently.\n\ntext = text.lower()\n\nRemoving PunctuationRemoves characters like !, ,, . which carry less meaning for models.\n\nimport string\n\ntext = text.translate(str.maketrans('', '', string.punctuation))\n\nRemoving StopwordsEliminates common words such as the, is, and, a that donâ€™t add semantic value.\n\nfrom nltk.corpus import stopwords\n\nwords = [w for w in text.split() if w not in stopwords.words('english')]\n\nStemmingReduces words to their root form â€” e.g., running â†’ run (uses heuristics).\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n\nstemmed = [stemmer.stem(word) for word in words]\n\nLemmatizationConverts words to their dictionary form â€” e.g., better â†’ good (uses vocabulary + context).\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nlemmatized = [lemmatizer.lemmatize(word) for word in words]\n\n\n\nB. Tokenization\n\nTokenization breaks text into smaller units (tokens) such as words, subwords, or sentences.\n\nTypes of Tokenizers\n\nType\n\nExample\n\nUsed In\n\nWord Tokenizer\n\n[\"I\", \"love\", \"AI\"]\n\nSimple models\n\nSubword Tokenizer\n\n[\"play\", \"##ing\"]\n\nTransformers (BERT, GPT)\n\nSentence Tokenizer\n\n[\"I love AI.\", \"Itâ€™s powerful.\"]\n\nSummarization, translation\n\nTokenization prepares text for embedding generation and model input.\n\n\n\nC. Embeddings\n\nEmbeddings convert words or sentences into numerical vectors that capture meaning.Words with similar meanings will have similar vector representations.\n\n Workflow of Embedding Generation\n\nInput text â†’ Tokenization\n\nTokens â†’ Converted to numeric IDs\n\nEach ID â†’ Mapped to an embedding vector (dense representation)\n\nThese vectors represent semantic meaning in continuous space.\n\nExample:\n\nking - man + woman â‰ˆ queen\n\n Common Embedding Models\n\nWord2Vec\n\nGloVe\n\nFastText\n\nBERT embeddings\n\nSentence Transformers\n\n\n\n2. Attention Mechanism\n\nDefinition\n\nThe Attention Mechanism helps models â€œfocusâ€ on the most relevant parts of input data when generating or understanding language.\n\nInstead of treating all words equally, attention assigns scores (weights) to highlight important words.\n\n\n\n How It Works\n\nStep 1ï¸: Calculate Attention Scores\n\nFor each word in a sentence:\n\nCompute how much attention it should give to other words.\n\nUse dot-product similarity between query and key vectors.\n\nFormula:[\\text{score}(Q, K) = Q \\cdot K^T]\n\nStep 2ï¸: Normalize Scores\n\nApply Softmax to get probabilities:[\\alpha_i = \\frac{e^{score_i}}{\\sum_j e^{score_j}}]\n\nStep 3ï¸  :  Adjust Focus Based on Scores\n\nMultiply the scores with value vectors to generate the context vector:[\\text{Attention}(Q,K,V) = \\text{Softmax}(QK^T)V]\n\nThis gives a weighted representation of information most relevant to the current word.\n\n\n\n3. Transformers\n\nDefinition\n\nTransformers are deep learning architectures that rely entirely on attention mechanisms (no recurrence or convolution).They are the backbone of models like BERT, GPT, T5, etc.\n\n\n\n\n\n\n\n\n\n\n\nComparison: Older vs Newer NLP Models\n\nModel Type\n\nExamples\n\nLimitations\n\nTransformers Fix\n\nTraditional (RNN, LSTM, GRU)\n\nSeq2Seq, LSTM\n\nSequential â†’ slow, hard to parallelize\n\nParallelized processing\n\nTransformers\n\nBERT, GPT\n\nUse self-attention â†’ process entire context simultaneously\n\nBetter context understanding, faster training\n\n\n\nHow Transformers Work (Step-by-Step)\n\nInput Tokenization â†’ Convert text to tokens\n\nEmbedding Layer â†’ Each token â†’ embedding vector\n\nPositional Encoding â†’ Adds order information (since transformer has no sequence memory)\n\nSelf-Attention Layer â†’ Calculates relationships between all tokens\n\nFeed Forward Network â†’ Processes each token independently\n\nNormalization + Residual Connections â†’ Stability and gradient flow\n\nStack Multiple Layers â†’ Deep understanding of context\n\nOutput Layer â†’ Generates or classifies text\n\n\n\n4. Text Similarity\n\nText similarity measures how close two pieces of text are in meaning.\n\nMethod\n\nDescription\n\nFormula / Idea\n\nCosine Similarity\n\nMeasures angle between two vectors\n\n(\\cos(\\theta) = \\frac{A \\cdot B}{\n\nJaccard Similarity\n\nMeasures overlap between sets\n\n(J(A,B) = \\frac{\n\nEuclidean Distance\n\nMeasures straight-line distance between vectors\n\n(d(A,B) = \\sqrt{\\sum (A_i - B_i)^2})\n\n\n\n5. Information Retrieval (IR)\n\nIR focuses on finding relevant documents from a large collection.\n\nðŸ”¹ Components\n\nDocument Representation\n\nConvert text into vectors using TF-IDF, BM25, or embeddings.\n\nIndexing\n\nStore document vectors in searchable structure.\n\nScoring & Ranking\n\nCalculate similarity between query and document.\n\nRank documents based on relevance score.\n\n\n\n6. Introduction to RAG (Retrieval-Augmented Generation)\n\nRAG combines retrieval (finding facts) and generation (creating natural language output).\n\nWorkflow:\n\nRetrieve relevant information from external sources.\n\nAugment the prompt with retrieved data.\n\nGenerate coherent responses using a generative model (like GPT).\n\nâœ… This improves factual accuracy and reduces hallucination.\n\n\n\n7. Retrieval Models\n\n Definition\n\nRetrieval models find the most relevant documents for a query.\n\nTypes\n\nTraditional (Sparse)\n\nDense (Neural)\n\n\n\n Traditional Models\n\n(a) TF-IDF\n\nRepresents text as word frequencies.\n\nAssigns higher weight to words that appear frequently in a document but rarely in others.\n\nLimitations:\n\nIgnores meaning/synonyms.\n\nSparse, large vectors.\n\n\n\n(b) BM25\n\nAn improved TF-IDF variant.\n\nConsiders term frequency saturation and document length normalization.\n\nBenefits:\n\nMore accurate for keyword-based search.\n\nStrong baseline in IR systems.\n\nLimitations:\n\nStill purely lexical â€” canâ€™t understand semantics.\n\n\n\nDense Retrieval Models\n\nUse neural networks to represent documents and queries in a shared embedding space.\n\nWorkflow:\n\nEncode query â†’ embedding\n\nEncode documents â†’ embeddings\n\nCompute cosine similarity\n\nRetrieve top-k most similar documents\n\nâœ… Benefits:\n\nUnderstand semantic meaning\n\nSupports contextual queries\n\nâš ï¸ Limitations:\n\nComputationally heavy\n\nRequires large training data\n\n\n\nAbsolutely ðŸ‘ â€” hereâ€™s a concise, clear version of the DPR (Dense Passage Retrieval) workflow, perfect for your documentation:\n\n\n\nDPR (Dense Passage Retrieval)\n\nDefinition:DPR is a neural retrieval model that finds semantically relevant passages for a given query using dense embeddings instead of keyword matching.\n\n\n\nWorkflow Steps\n\nInput:\n\nTakes a query (e.g., â€œWho founded PostgreSQL?â€)\n\nAnd a collection of passages/documents\n\nEncoders:\n\nQuery Encoder â†’ converts query into a dense vector\n\nPassage Encoder â†’ converts each passage into a dense vector\n\nBoth are typically BERT-based and map text into the same vector space.\n\nCompute Similarity:\n\nCalculate cosine similarity between query and passage vectors\n\nHigher score = higher semantic relevance\n\nRank Documents:\n\nRank all passages based on similarity scores\n\nSelect the Top-K most relevant ones\n\nOutput:\n\nReturns top passages as retrieved context for RAG or QA systems\n\nThese passages are used by the generator (like GPT) to produce final, factual responses.\n\n\n\n8. Generative Models\n\nDefinition\n\nGenerative models produce new text by predicting the next token based on previous context (e.g., GPT, T5, LLaMA).\n\nAdvantages over Retrieval Models\n\nRetrieval Models\n\nGenerative Models\n\nFetch existing data\n\nGenerate new data\n\nKeyword or embedding-based\n\nContext and pattern-based\n\nStatic knowledge\n\nCan generalize & reason\n\nLimited creativity\n\nHighly flexible & adaptive\n\n\n\n Workflow of Generative Models\n\nTokenize input text\n\nPass through transformer layers\n\nCompute attention & hidden states\n\nPredict next token (autoregressive generation)\n\nRepeat until complete output\n\n\n\nComparison: Retrieval vs Generative\n\nFeature\n\nRetrieval\n\nGenerative\n\nOutput\n\nFactual text from memory\n\nNew content\n\nContext\n\nNarrow\n\nDeep contextual\n\nExample\n\nSearch engine\n\nChatGPT\n\nLimitation\n\nCanâ€™t create new info\n\nMay hallucinate\n\nBest Use\n\nInformation lookup\n\nConversation, summarization\n\n\n\n\n\n\n\n9. Combining Retrieval + Generation (RAG Systems)\n\n Why Combine?\n\nBecause:\n\nRetrieval ensures accuracy & factual grounding\n\nGeneration ensures natural, coherent language\n\n\n\nSteps in Retrieval\n\nRepresent query\n\nSearch index\n\nScore and rank results\n\nReturn top relevant documents\n\nSteps in Generation\n\nTake retrieved documents\n\nAppend to user query (context window)\n\nGenerate text response using LLM\n\nRAG = Retrieval + Generation\n\nQuery â†’ Retrieve supporting facts\n\nFused prompt â†’ Pass to LLM\n\nLLM â†’ Generate answer with evidence\n\nâœ… Result: Factual, context-aware, and fluent outputs.\n\n",
        "detectedPatterns": [
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value",
          "key-value"
        ],
        "keyValuePairs": {
          "Step 1ï¸": "Calculate Attention Scores",
          "Formula": "[\\text{score}(Q, K) = Q \\cdot K^T]",
          "Step 2ï¸": "Normalize Scores",
          "Apply Softmax to get probabilities": "[\\alpha_i = \\frac{e^{score_i}}{\\sum_j e^{score_j}}]",
          "Step 3ï¸": "Adjust Focus Based on Scores",
          "Multiply the scores with value vectors to generate the context vector": "[\\text{Attention}(Q,K,V) = \\text{Softmax}(QK^T)V]",
          "Comparison": "Retrieval vs Generative",
          "Definition": "DPR is a neural retrieval model that finds semantically relevant passages for a given query using dense embeddings instead of keyword matching.",
          "âœ… Result": "Factual, context-aware, and fluent outputs."
        }
      }
    }
  ]
}